{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0c8211",
   "metadata": {},
   "source": [
    "Run this notebook to\n",
    "\n",
    "- xx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa373f",
   "metadata": {},
   "source": [
    "## Import stuff and set parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72988d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e09e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr \n",
    "from itertools import chain, zip_longest\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import mne\n",
    "from mne.stats import permutation_t_test\n",
    "mne.set_log_level('warning') \n",
    "\n",
    "#%matplotlib qt\n",
    "%matplotlib inline\n",
    "\n",
    "input_dir = 'TaskresponseEpochsMastoids'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subj_eeg(path, file, downsample=None):\n",
    "    fp = os.path.join(path, '%s-epo.fif' % file)\n",
    "    print('>>> Loading %s' % fp)\n",
    "    epochs = mne.read_epochs(fp, preload=True)\n",
    "    if downsample is not None:\n",
    "        epochs = epochs.resample(downsample)\n",
    "    return epochs\n",
    "\n",
    "def load_all_eeg(path, files, downsample=None):\n",
    "    subject_epochs = [load_subj_eeg(path, file, downsample=downsample) for file in files]\n",
    "    epochs = mne.epochs.concatenate_epochs(subject_epochs)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02287825",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21] # 14 was excluded due to noise\n",
    "\n",
    "sessions = [1, 2]\n",
    "\n",
    "partners = ['\"overconfident\"', '\"underconfident\"']\n",
    "\n",
    "epoch_start = 0.1\n",
    "epoch_end = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b374f",
   "metadata": {},
   "source": [
    "## Linear regression predicting confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce76ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_lr_scores = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "#     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "#     print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    # independent variables\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    X = epochs.get_data()\n",
    "    \n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    \n",
    "    # collapse across trials and timepoints\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "    X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y = np.repeat(y, n_timepoints)\n",
    "    \n",
    "    # fit regression\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    score = reg.score(X, y)\n",
    "#     print(score)\n",
    "    \n",
    "#     # regression betas\n",
    "#     betas = reg.coef_\n",
    "#     #print(betas)\n",
    "    \n",
    "#     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "    all_lr_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d638a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Average R-squared linear regression: %f' % (sum(all_lr_scores) / len(all_lr_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237e10a",
   "metadata": {},
   "source": [
    "## Cross-validation random split across timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c27e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_cv_scores = []\n",
    "\n",
    "for subject in participant_numbers:\n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###### INDEPENDENT VARIABLES\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "    X = epochs.get_data()\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "\n",
    "    \n",
    "    \n",
    "    ###### DEPENDENT VARIABLES\n",
    "    ## this version is splitting data randomly by timepoints - topoplots etc look v similar\n",
    "    # 50% training data, 50% test data\n",
    "    X = X.reshape(-1, n_channels)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    y = np.repeat(y, n_timepoints)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "    \n",
    "\n",
    "    \n",
    "    ###### TRAIN CLASSIFIER\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train, y_train) #training the algorithm\n",
    "\n",
    "    # regression betas\n",
    "    betas = regressor.coef_\n",
    "    \n",
    "    # predict y from X_test\n",
    "    y_pred = regressor.predict(X_test)    \n",
    "    \n",
    "    \n",
    "    ###### PLOT\n",
    "    # plot topoplot with beta weights\n",
    "    mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "    # plot topoplot with sensor projections\n",
    "    # get out sensor projections (see Parra et al paper)\n",
    "    y = y_pred\n",
    "    x = X_test\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show();\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")\n",
    "    \n",
    "#     # plot normal ERP over time\n",
    "#     across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "#     average_across_epochs_erp = across_epochs_erp.mean(axis=0) # compute the average across electrodes \n",
    "#     x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "#     plt.plot(x_axis, average_across_epochs_erp)\n",
    "#     plt.show()\n",
    "#     print(\"Average ERP (not weighted) \\n\\n\")\n",
    "\n",
    "    \n",
    "    print('R-squared:', r2_score(y_test, y_pred))  \n",
    "    all_cv_scores.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc2365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Average R-squared cross validation random timepoint split: %f' % (sum(all_cv_scores) / len(all_cv_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c06df",
   "metadata": {},
   "source": [
    "## Cross-validation training on odd (even) trials, testing on even (odd) trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff573b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores_test = []\n",
    "all_scores_test_trialwise = []\n",
    "all_scores_train = []\n",
    "all_scores_train_trialwise = []\n",
    "\n",
    "\n",
    "for subject in participant_numbers:\n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### INDEPENDENT VARIABLE ######################### \n",
    "    ######################################################################\n",
    "    \n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "    X = epochs.get_data()\n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels) (600, 127, 64)\n",
    "    \n",
    "    # divide into odd and even trials (3D array) (300, 127, 64)\n",
    "    X_odd_trials = X[1::2, :, :] # select every second epoch (start:stop:step, :, :)\n",
    "    X_even_trials = X[0::2, :, :] # select every other second epoch\n",
    "    \n",
    "    # collapse across trials and timepoints (2D array) (triple checked this and it is correctly sorted as e1t1, e1t2, e1t3, ...)\n",
    "    X_odd_per_timepoint = X_odd_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "    X_even_per_timepoint = X_even_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    \n",
    "    ######################################################################\n",
    "    ######################## DEPENDENT VARIABLE ########################## \n",
    "    ######################################################################\n",
    "    \n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    \n",
    "    # divide into odd and even trials\n",
    "    y_odd_trials = y[1::2]\n",
    "    y_even_trials = y[0::2]\n",
    "    \n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y_odd_per_timepoint = np.repeat(y_odd_trials, n_timepoints)\n",
    "    y_even_per_timepoint = np.repeat(y_even_trials, n_timepoints)\n",
    "     \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON ODD TRIALS ####################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on odd trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_odd_trials\n",
    "    X_train_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_even_trials\n",
    "    X_test_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_odd_trials\n",
    "    y_train_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_even_trials\n",
    "    y_test_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_   \n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "    # OPTION 1\n",
    "    # use predict function on individual timepoints\n",
    "    y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "    \n",
    "    # OPTION 2\n",
    "    # use betas and intercept on individual timepoints   \n",
    "    y_pred_per_timepoint_2 = np.sum(X_test_per_timepoint * betas, axis=1) + intercept \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred_2 = np.mean(y_pred_per_timepoint_2.reshape(-1, n_timepoints), axis=1)\n",
    "    \n",
    "    # OPTION 3\n",
    "    # use predict function on epoch average\n",
    "    epoch_y_pred_3 = regressor.predict(np.mean(X_test_trials, axis=1)) # take mean voltage along n_times axis (n_epochs, n_times, n_channels)\n",
    "    \n",
    "    # OPTION 4\n",
    "    # use betas and intercept on epoch average\n",
    "    epoch_y_pred_4 = np.sum(np.mean(X_test_trials, axis=1) * betas, axis=1) + intercept \n",
    "\n",
    "#     # sanity check - all options give the same predicted y per epoch\n",
    "#     print(epoch_y_pred_1[0:6])\n",
    "#     print(epoch_y_pred_2[0:6])\n",
    "#     print(epoch_y_pred_3[0:6])\n",
    "#     print(epoch_y_pred_4[0:6])\n",
    "\n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "#     # sanity check - manual computation gives same R-squared\n",
    "#     actual_minus_predicted = sum((y_test_per_timepoint - y_pred_per_timepoint_1)**2)\n",
    "#     actual_minus_actual_mean = sum((y_test_per_timepoint - y_test_per_timepoint.mean())**2)\n",
    "#     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "#     print('R-squared timepoint data:', r2)\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "#     # sanity check - manual computation gives same R-squared\n",
    "#     actual_minus_predicted = sum((y_test_trials - epoch_y_pred_1)**2)\n",
    "#     actual_minus_actual_mean = sum((y_test_trials - y_test_trials.mean())**2)\n",
    "#     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "#     print('R-squared trial data:', r2)\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "    plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "\n",
    "    ####################### PLOTTING ####################### \n",
    "    \n",
    "    # plot topoplot with beta weights\n",
    "    mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "    \n",
    "    # plot topoplot with sensor projections (see Parra et al paper for equation)\n",
    "    y = y_pred_per_timepoint\n",
    "    x = X_test_per_timepoint\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "    \n",
    "#     # sanity check - plotting with trial data rather than timepoint data gives the same plot\n",
    "#     y = epoch_y_pred\n",
    "#     x = np.mean(X_test_trials, axis=1) \n",
    "#     a =  y.T@x * ( 1 / (y.T@y) )\n",
    "#     mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "#     print(\"Sensor projections trial data \\n\\n\")\n",
    "    \n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show()\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON EVEN TRIALS ###################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on even trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_even_trials\n",
    "    X_train_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_odd_trials\n",
    "    X_test_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_even_trials\n",
    "    y_train_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_odd_trials\n",
    "    y_test_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "    # use predict function on individual timepoints\n",
    "    y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "#     # sanity check - manual computation gives same R-squared\n",
    "#     actual_minus_predicted = sum((y_test_per_timepoint - y_pred_per_timepoint_1)**2)\n",
    "#     actual_minus_actual_mean = sum((y_test_per_timepoint - y_test_per_timepoint.mean())**2)\n",
    "#     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "#     print('R-squared timepoint data:', r2)\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "#     # sanity check - manual computation gives same R-squared\n",
    "#     actual_minus_predicted = sum((y_test_trials - epoch_y_pred_1)**2)\n",
    "#     actual_minus_actual_mean = sum((y_test_trials - y_test_trials.mean())**2)\n",
    "#     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "#     print('R-squared trial data:', r2)\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "    plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "\n",
    "    ####################### PLOTTING ####################### \n",
    "    \n",
    "    # plot topoplot with beta weights\n",
    "    mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "    \n",
    "    # plot topoplot with sensor projections (see Parra et al paper for equation)\n",
    "    y = y_pred_per_timepoint\n",
    "    x = X_test_per_timepoint\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "#     # shuffle ys --> gives same topoplot because my predictions are basically random and then this just plots the average voltage which happens to look like P3\n",
    "#     y = y_pred_per_timepoint\n",
    "#     np.random.shuffle(y)\n",
    "#     x = X_test_per_timepoint\n",
    "#     a =  y.T@x * ( 1 / (y.T@y) )\n",
    "#     mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "#     print(\"Sensor projections shuffled ys \\n\\n\")\n",
    "    \n",
    "#     # shuffle xs --> gives random topoplot\n",
    "#     y = y_pred_per_timepoint\n",
    "#     x = X_test_per_timepoint\n",
    "#     def shuffle_along_axis(a, axis):\n",
    "#         idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "#         return np.take_along_axis(a,idx,axis=axis)\n",
    "#     x = shuffle_along_axis(x, 1)\n",
    "#     a =  y.T@x * ( 1 / (y.T@y) )\n",
    "#     mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "#     print(\"Sensor projections shuffled xs \\n\\n\")\n",
    "\n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show()\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaa8a6",
   "metadata": {},
   "source": [
    "R-squared timepoint wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared test timepoint-wise: %f' % (sum(all_scores_test) / len(all_scores_test),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared train timepoint-wise: %f' % (sum(all_scores_train) / len(all_scores_train),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfce5b",
   "metadata": {},
   "source": [
    "R-squared trial wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared test trialwise: %f' % (sum(all_scores_test_trialwise) / len(all_scores_test_trialwise),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared train trialwise: %f' % (sum(all_scores_train_trialwise) / len(all_scores_train_trialwise),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fb51a",
   "metadata": {},
   "source": [
    "### Add partner condition as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d301b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores_test = []\n",
    "all_scores_test_trialwise = []\n",
    "all_scores_train = []\n",
    "all_scores_train_trialwise = []\n",
    "\n",
    "\n",
    "for subject in participant_numbers:\n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### INDEPENDENT VARIABLE ######################### \n",
    "    ######################################################################\n",
    "    \n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "    X = epochs.get_data()\n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels) (600, 127, 64)\n",
    "    \n",
    "    # divide into odd and even trials (3D array) (300, 127, 64)\n",
    "    X_odd_trials = X[1::2, :, :] # select every second epoch (start:stop:step, :, :)\n",
    "    X_even_trials = X[0::2, :, :] # select every other second epoch\n",
    "    \n",
    "    # collapse across trials and timepoints (2D array) (triple checked this and it is correctly sorted as e1t1, e1t2, e1t3, ...)\n",
    "    X_odd_per_timepoint = X_odd_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "    X_even_per_timepoint = X_even_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    \n",
    "    # get partner for every trial as a numeric predictor\n",
    "    partner = epochs.metadata['partner'].to_numpy()\n",
    "    partner[partner=='underconfident']= -1\n",
    "    partner[partner=='overconfident']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    partner_odd_trials = partner[1::2]\n",
    "    partner_even_trials = partner[0::2]\n",
    "\n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where partner stays the same\n",
    "    partner_odd_per_timepoint = np.repeat(partner_odd_trials, n_timepoints)\n",
    "    partner_even_per_timepoint = np.repeat(partner_even_trials, n_timepoints)\n",
    "    \n",
    "    # add partner as a predictor variable\n",
    "    X_odd_per_timepoint = np.column_stack([X_odd_per_timepoint,partner_odd_per_timepoint])\n",
    "    X_even_per_timepoint = np.column_stack([X_even_per_timepoint,partner_even_per_timepoint])\n",
    "\n",
    "    ######################################################################\n",
    "    ######################## DEPENDENT VARIABLE ########################## \n",
    "    ######################################################################\n",
    "    \n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    \n",
    "    # divide into odd and even trials\n",
    "    y_odd_trials = y[1::2]\n",
    "    y_even_trials = y[0::2]\n",
    "    \n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y_odd_per_timepoint = np.repeat(y_odd_trials, n_timepoints)\n",
    "    y_even_per_timepoint = np.repeat(y_even_trials, n_timepoints)\n",
    "     \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON ODD TRIALS ####################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on odd trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_odd_trials\n",
    "    X_train_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_even_trials\n",
    "    X_test_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_odd_trials\n",
    "    y_train_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_even_trials\n",
    "    y_test_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "        \n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "    # use predict function on individual timepoints\n",
    "    y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "    \n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "    sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "    plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "\n",
    "    ####################### PLOTTING ####################### \n",
    "    \n",
    "    # plot topoplot with beta weights minus partner beta\n",
    "    betas = betas[:-1]\n",
    "    mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "    # plot topoplot with sensor projections (see Parra et al paper for equation)\n",
    "    y = y_pred_per_timepoint\n",
    "    x = X_test_per_timepoint[:, :-1]\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "    \n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show()\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON EVEN TRIALS ###################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on even trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_even_trials\n",
    "    X_train_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_odd_trials\n",
    "    X_test_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_even_trials\n",
    "    y_train_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_odd_trials\n",
    "    y_test_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "    # use predict function on individual timepoints\n",
    "    y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "    sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "    plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "\n",
    "    ####################### PLOTTING ####################### \n",
    "\n",
    "    # plot topoplot with beta weights minus partner beta\n",
    "    betas = betas[:-1]\n",
    "    mne.viz.plot_topomap(data=betas , pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "    \n",
    "    # plot topoplot with sensor projections (see Parra et al paper for equation) minus partner parameter\n",
    "    y = y_pred_per_timepoint\n",
    "    x = X_test_per_timepoint[:, :-1]\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show()\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b668cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test timepoint-wise: %f' % (sum(all_scores_test) / len(all_scores_test),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train timepoint-wise: %f' % (sum(all_scores_train) / len(all_scores_train),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test trialwise: %f' % (sum(all_scores_test_trialwise) / len(all_scores_test_trialwise),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train trialwise: %f' % (sum(all_scores_train_trialwise) / len(all_scores_train_trialwise),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110f418",
   "metadata": {},
   "source": [
    "### Add partner and task condition as predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758f5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores_test = []\n",
    "all_scores_test_trialwise = []\n",
    "all_scores_train = []\n",
    "all_scores_train_trialwise = []\n",
    "\n",
    "\n",
    "for subject in participant_numbers:\n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### INDEPENDENT VARIABLE ######################### \n",
    "    ######################################################################\n",
    "    \n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "    X = epochs.get_data()\n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels) (600, 127, 64)\n",
    "    \n",
    "    # divide into odd and even trials (3D array) (300, 127, 64)\n",
    "    X_odd_trials = X[1::2, :, :] # select every second epoch (start:stop:step, :, :)\n",
    "    X_even_trials = X[0::2, :, :] # select every other second epoch\n",
    "    \n",
    "    # collapse across trials and timepoints (2D array) (triple checked this and it is correctly sorted as e1t1, e1t2, e1t3, ...)\n",
    "    X_odd_per_timepoint = X_odd_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "    X_even_per_timepoint = X_even_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    \n",
    "    # get partner for every trial as a numeric predictor\n",
    "    partner = epochs.metadata['partner'].to_numpy()\n",
    "    partner[partner=='underconfident']= -1\n",
    "    partner[partner=='overconfident']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    partner_odd_trials = partner[1::2]\n",
    "    partner_even_trials = partner[0::2]\n",
    "\n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where partner stays the same\n",
    "    partner_odd_per_timepoint = np.repeat(partner_odd_trials, n_timepoints)\n",
    "    partner_even_per_timepoint = np.repeat(partner_even_trials, n_timepoints)\n",
    "    \n",
    "    # add partner as a predictor variable\n",
    "    X_odd_per_timepoint = np.column_stack([X_odd_per_timepoint,partner_odd_per_timepoint])\n",
    "    X_even_per_timepoint = np.column_stack([X_even_per_timepoint,partner_even_per_timepoint])\n",
    "    \n",
    "    \n",
    "    # get condition for every trial as a numeric predictor\n",
    "    condition = epochs.metadata['condition'].to_numpy()\n",
    "    condition[condition=='ns']= -1\n",
    "    condition[condition=='s']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    condition_odd_trials = condition[1::2]\n",
    "    condition_even_trials = condition[0::2]\n",
    "\n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where partner stays the same\n",
    "    condition_odd_per_timepoint = np.repeat(condition_odd_trials, n_timepoints)\n",
    "    condition_even_per_timepoint = np.repeat(condition_even_trials, n_timepoints)\n",
    "    \n",
    "    # add condition as a predictor variable\n",
    "    X_odd_per_timepoint = np.column_stack([X_odd_per_timepoint,condition_odd_per_timepoint])\n",
    "    X_even_per_timepoint = np.column_stack([X_even_per_timepoint,condition_even_per_timepoint])\n",
    "\n",
    "    ######################################################################\n",
    "    ######################## DEPENDENT VARIABLE ########################## \n",
    "    ######################################################################\n",
    "    \n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    \n",
    "    # divide into odd and even trials\n",
    "    y_odd_trials = y[1::2]\n",
    "    y_even_trials = y[0::2]\n",
    "    \n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y_odd_per_timepoint = np.repeat(y_odd_trials, n_timepoints)\n",
    "    y_even_per_timepoint = np.repeat(y_even_trials, n_timepoints)\n",
    "     \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON ODD TRIALS ####################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on odd trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_odd_trials\n",
    "    X_train_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_even_trials\n",
    "    X_test_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_odd_trials\n",
    "    y_train_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_even_trials\n",
    "    y_test_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "#     # use predict function on individual timepoints\n",
    "#     y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "#     # then average predicted values within an epoch\n",
    "#     epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "    # use betas and intercept on individual timepoints   \n",
    "    y_pred_per_timepoint = np.sum(X_test_per_timepoint * betas, axis=1) + intercept \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "    \n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "#     sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "#     plt.xlabel('Reported confidence')\n",
    "#     plt.ylabel('Classifier prediction')\n",
    "#     plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "#     plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    # plot y against predicted y\n",
    "#     sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "#     plt.xlabel('Reported confidence')\n",
    "#     plt.ylabel('Classifier prediction')\n",
    "#     plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "#     plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "\n",
    "    ####################### PLOTTING ####################### \n",
    "    \n",
    "    # plot topoplot with beta weights minus partner and condition betas\n",
    "    betas = betas[:-2]\n",
    "    mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "    # plot topoplot with sensor projections (see Parra et al paper for equation) minus partner and condition betas\n",
    "    y = y_pred_per_timepoint\n",
    "    x = X_test_per_timepoint[:, :-2]\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "    \n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show()\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON EVEN TRIALS ###################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on even trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_even_trials\n",
    "    X_train_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_odd_trials\n",
    "    X_test_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_even_trials\n",
    "    y_train_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_odd_trials\n",
    "    y_test_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "#     # use predict function on individual timepoints\n",
    "#     y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "#     # then average predicted values within an epoch\n",
    "#     epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "    # use betas and intercept on individual timepoints   \n",
    "    y_pred_per_timepoint = np.sum(X_test_per_timepoint * betas, axis=1) + intercept \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "#     sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "#     plt.xlabel('Reported confidence')\n",
    "#     plt.ylabel('Classifier prediction')\n",
    "#     plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "#     plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    # plot y against predicted y\n",
    "#     sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "#     plt.xlabel('Reported confidence')\n",
    "#     plt.ylabel('Classifier prediction')\n",
    "#     plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "#     plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "\n",
    "    ####################### PLOTTING ####################### \n",
    "\n",
    "    # plot topoplot with beta weights minus partner and condition betas\n",
    "    betas = betas[:-2]\n",
    "    mne.viz.plot_topomap(data=betas , pos=epochs.info)\n",
    "    print(\"Beta weights for each channel \\n\\n\")\n",
    "    \n",
    "    # plot topoplot with sensor projections (see Parra et al paper for equation) minus partner and condition parameters\n",
    "    y = y_pred_per_timepoint\n",
    "    x = X_test_per_timepoint[:, :-2]\n",
    "    a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "    # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "    across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "    average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "    x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    plt.plot(x_axis, average_weighted_erp)\n",
    "    plt.show()\n",
    "    print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d3fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test timepoint-wise: %f' % (sum(all_scores_test) / len(all_scores_test),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f177438",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train timepoint-wise: %f' % (sum(all_scores_train) / len(all_scores_train),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test trialwise: %f' % (sum(all_scores_test_trialwise) / len(all_scores_test_trialwise),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train trialwise: %f' % (sum(all_scores_train_trialwise) / len(all_scores_train_trialwise),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b53ddf",
   "metadata": {},
   "source": [
    "### Regression with only partner and task condition but without EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5583720",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_test = []\n",
    "all_scores_train = []\n",
    "\n",
    "for subject in participant_numbers:\n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### INDEPENDENT VARIABLE ######################### \n",
    "    ######################################################################\n",
    "\n",
    "    # get partner for every trial as a numeric predictor\n",
    "    partner = epochs.metadata['partner'].to_numpy()\n",
    "    partner[partner=='underconfident']= -1\n",
    "    partner[partner=='overconfident']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    partner_odd_trials = partner[1::2]\n",
    "    partner_even_trials = partner[0::2]\n",
    "    \n",
    "    # get condition for every trial as a numeric predictor\n",
    "    condition = epochs.metadata['condition'].to_numpy()\n",
    "    condition[condition=='ns']= -1\n",
    "    condition[condition=='s']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    condition_odd_trials = condition[1::2]\n",
    "    condition_even_trials = condition[0::2]\n",
    "\n",
    "    \n",
    "    # add partner and condition as predictor variables\n",
    "    X_odd_trials = np.column_stack([partner_odd_trials,condition_odd_trials])\n",
    "    X_even_trials = np.column_stack([partner_even_trials,condition_even_trials])\n",
    "\n",
    "    ######################################################################\n",
    "    ######################## DEPENDENT VARIABLE ########################## \n",
    "    ######################################################################\n",
    "    \n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    \n",
    "    # divide into odd and even trials\n",
    "    y_odd_trials = y[1::2]\n",
    "    y_even_trials = y[0::2]\n",
    "     \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON ODD TRIALS ####################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on odd trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_odd_trials\n",
    "    X_test_trials = X_even_trials\n",
    "       \n",
    "    y_train_trials = y_odd_trials \n",
    "    y_test_trials = y_even_trials\n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_trials, y_train_trials)\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "    # use predict function on individual timepoints\n",
    "    y_pred = regressor.predict(X_test_trials) \n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, y_pred))\n",
    "    sns.regplot(x=y_test_trials, y=y_pred, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    y_pred_train = regressor.predict(X_train_trials) \n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, y_pred_train))\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_train_trials, y=y_pred_train, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "    plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON EVEN TRIALS ###################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on even trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_even_trials\n",
    "    X_test_trials = X_odd_trials\n",
    "       \n",
    "    y_train_trials = y_even_trials \n",
    "    y_test_trials = y_odd_trials\n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_trials, y_train_trials)\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test \n",
    "    # use predict function on individual timepoints\n",
    "    y_pred = regressor.predict(X_test_trials) \n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, y_pred))\n",
    "    sns.regplot(x=y_test_trials, y=y_pred, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    y_pred_train = regressor.predict(X_train_trials) \n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, y_pred_train))\n",
    "    # plot y against predicted y\n",
    "    sns.regplot(x=y_train_trials, y=y_pred_train, scatter_kws={'alpha':0.5})\n",
    "    plt.xlabel('Reported confidence')\n",
    "    plt.ylabel('Classifier prediction')\n",
    "    plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "    plt.show();\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test: %f' % (sum(all_scores_test) / len(all_scores_test),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3599790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train: %f' % (sum(all_scores_train) / len(all_scores_train),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82813bb6",
   "metadata": {},
   "source": [
    "### Regression with partner and task conditions as predictors for trainig the classifier but then only use EEG betas for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_test = []\n",
    "all_scores_test_trialwise = []\n",
    "all_scores_train = []\n",
    "all_scores_train_trialwise = []\n",
    "\n",
    "\n",
    "for subject in participant_numbers:\n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### INDEPENDENT VARIABLE ######################### \n",
    "    ######################################################################\n",
    "    \n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "    X = epochs.get_data()\n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels) (600, 127, 64)\n",
    "    \n",
    "    # divide into odd and even trials (3D array) (300, 127, 64)\n",
    "    X_odd_trials = X[1::2, :, :] # select every second epoch (start:stop:step, :, :)\n",
    "    X_even_trials = X[0::2, :, :] # select every other second epoch\n",
    "    \n",
    "    # collapse across trials and timepoints (2D array) (triple checked this and it is correctly sorted as e1t1, e1t2, e1t3, ...)\n",
    "    X_odd_per_timepoint = X_odd_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "    X_even_per_timepoint = X_even_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    \n",
    "    # get partner for every trial as a numeric predictor\n",
    "    partner = epochs.metadata['partner'].to_numpy()\n",
    "    partner[partner=='underconfident']= -1\n",
    "    partner[partner=='overconfident']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    partner_odd_trials = partner[1::2]\n",
    "    partner_even_trials = partner[0::2]\n",
    "\n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where partner stays the same\n",
    "    partner_odd_per_timepoint = np.repeat(partner_odd_trials, n_timepoints)\n",
    "    partner_even_per_timepoint = np.repeat(partner_even_trials, n_timepoints)\n",
    "    \n",
    "    # add partner as a predictor variable\n",
    "    X_odd_per_timepoint = np.column_stack([X_odd_per_timepoint,partner_odd_per_timepoint])\n",
    "    X_even_per_timepoint = np.column_stack([X_even_per_timepoint,partner_even_per_timepoint])\n",
    "    \n",
    "    \n",
    "    # get condition for every trial as a numeric predictor\n",
    "    condition = epochs.metadata['condition'].to_numpy()\n",
    "    condition[condition=='ns']= -1\n",
    "    condition[condition=='s']= 1\n",
    "\n",
    "    # divide into odd and even trials\n",
    "    condition_odd_trials = condition[1::2]\n",
    "    condition_even_trials = condition[0::2]\n",
    "\n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where partner stays the same\n",
    "    condition_odd_per_timepoint = np.repeat(condition_odd_trials, n_timepoints)\n",
    "    condition_even_per_timepoint = np.repeat(condition_even_trials, n_timepoints)\n",
    "    \n",
    "    # add condition as a predictor variable\n",
    "    X_odd_per_timepoint = np.column_stack([X_odd_per_timepoint,condition_odd_per_timepoint])\n",
    "    X_even_per_timepoint = np.column_stack([X_even_per_timepoint,condition_even_per_timepoint])\n",
    "\n",
    "    ######################################################################\n",
    "    ######################## DEPENDENT VARIABLE ########################## \n",
    "    ######################################################################\n",
    "    \n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    \n",
    "    # divide into odd and even trials\n",
    "    y_odd_trials = y[1::2]\n",
    "    y_even_trials = y[0::2]\n",
    "    \n",
    "    # make from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y_odd_per_timepoint = np.repeat(y_odd_trials, n_timepoints)\n",
    "    y_even_per_timepoint = np.repeat(y_even_trials, n_timepoints)\n",
    "     \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON ODD TRIALS ####################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on odd trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_odd_trials\n",
    "    X_train_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_even_trials\n",
    "    X_test_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_odd_trials\n",
    "    y_train_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_even_trials\n",
    "    y_test_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test without task and partner betas   \n",
    "    # use betas and intercept on individual timepoints   \n",
    "    y_pred_per_timepoint = np.sum(X_test_per_timepoint[:, :-2] * betas[:-2], axis=1) + intercept \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "    \n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    ####################### TRAINING ON EVEN TRIALS ###################### \n",
    "    ######################################################################\n",
    "    \n",
    "    print('\\033[1m' + \"\\n\\n ______ train on even trials ______\\n\")\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    X_train_trials = X_even_trials\n",
    "    X_train_per_timepoint = X_even_per_timepoint\n",
    "    \n",
    "    X_test_trials = X_odd_trials\n",
    "    X_test_per_timepoint = X_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    y_train_trials = y_even_trials\n",
    "    y_train_per_timepoint = y_even_per_timepoint\n",
    "    \n",
    "    y_test_trials = y_odd_trials\n",
    "    y_test_per_timepoint = y_odd_per_timepoint\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train classifier\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "    betas = regressor.coef_\n",
    "    intercept = regressor.intercept_\n",
    "    \n",
    "    # predict y from X_test without task and partner betas   \n",
    "    # use betas and intercept on individual timepoints   \n",
    "    y_pred_per_timepoint = np.sum(X_test_per_timepoint[:, :-2] * betas[:-2], axis=1) + intercept \n",
    "    # then average predicted values within an epoch\n",
    "    epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "    print('\\033[1m' + \"\\n test data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    \n",
    "    # R squared for trials\n",
    "    print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "    \n",
    "\n",
    "    # R squared for training data\n",
    "    print('\\033[1m' + \"\\n train data\")\n",
    "    print('\\033[0m')\n",
    "    # R squared for timepoints\n",
    "    y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "    print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    # R squared for trials\n",
    "    epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "    print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "    \n",
    "    all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "    all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "    all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train)) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731068f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test timepoint-wise: %f' % (sum(all_scores_test) / len(all_scores_test),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9df06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train timepoint-wise: %f' % (sum(all_scores_train) / len(all_scores_train),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared test trialwise: %f' % (sum(all_scores_test_trialwise) / len(all_scores_test_trialwise),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ecda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Average R-squared train trialwise: %f' % (sum(all_scores_train_trialwise) / len(all_scores_train_trialwise),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937e34",
   "metadata": {},
   "source": [
    "## Ridge regression (regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_lr_scores_ridge = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "#     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "#     print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    # independent variables\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    X = epochs.get_data()\n",
    "    \n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    \n",
    "    # collapse across trials and timepoints\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "    X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y = np.repeat(y, n_timepoints)\n",
    "    \n",
    "    # fit regression\n",
    "    reg = Ridge(alpha=.5).fit(X, y)\n",
    "    score = reg.score(X, y)\n",
    "#     print(score)\n",
    "    \n",
    "#     # regression betas\n",
    "#     betas = reg.coef_\n",
    "#     #print(betas)\n",
    "    \n",
    "#     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "    all_lr_scores_ridge.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f49753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared ridge regression: %f' % (sum(all_lr_scores_ridge) / len(all_lr_scores_ridge),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d29dd",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92840bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_lg_scores = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "#     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "#     print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    # independent variables\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    X = epochs.get_data()\n",
    "    \n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    \n",
    "    # collapse across trials and timepoints\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "    X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    # make it binary\n",
    "    med_y = np.median(y)\n",
    "    y[y<=med_y] = -1\n",
    "    y[y>med_y] = 1\n",
    "    \n",
    "    # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y = np.repeat(y, n_timepoints)\n",
    "    \n",
    "    # fit regression\n",
    "    reg = LogisticRegression().fit(X, y)\n",
    "    score = reg.score(X, y)\n",
    "#     print(score)\n",
    "    \n",
    "#     # regression betas\n",
    "#     betas = reg.coef_\n",
    "#     #print(betas)\n",
    "    \n",
    "#     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "    all_lg_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Average accuracy logistic regression: %f' % (sum(all_lg_scores) / len(all_lg_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c829c18",
   "metadata": {},
   "source": [
    "## Multinomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e86ab",
   "metadata": {},
   "source": [
    "confidence quintile split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc22b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_mn_scores = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "#     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "#     print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    # independent variables\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    X = epochs.get_data()\n",
    "    \n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    \n",
    "    # collapse across trials and timepoints\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "    X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    # make it categorical\n",
    "    quintiles = np.percentile( y, [20,40,60,80] )    \n",
    "    y = 5 - (quintiles[:,None] >=  y).sum(0)\n",
    "    \n",
    "    # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y = np.repeat(y, n_timepoints)\n",
    "    \n",
    "    # fit regression\n",
    "    reg = LogisticRegression().fit(X, y)\n",
    "    score = reg.score(X, y)\n",
    "#     print(score)\n",
    "    \n",
    "#     # regression betas\n",
    "#     betas = reg.coef_\n",
    "#     #print(betas)\n",
    "    \n",
    "#     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "    all_mn_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Average accuracy multinomial regression using quintiles: %f' % (sum(all_mn_scores) / len(all_mn_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb897629",
   "metadata": {},
   "source": [
    "confidence quartile split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788717d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_mn_scores = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "#     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "#     print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    # independent variables\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    X = epochs.get_data()\n",
    "    \n",
    "    n_epochs = X.shape[0]\n",
    "    n_channels = X.shape[1]\n",
    "    n_timepoints = X.shape[2]\n",
    "    \n",
    "    # collapse across trials and timepoints\n",
    "    X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "    X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    # make it categorical\n",
    "    quartiles = np.percentile( y, [25,50,75] )    \n",
    "    y = 4 - (quartiles[:,None] >=  y).sum(0)\n",
    "    \n",
    "    # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "    y = np.repeat(y, n_timepoints)\n",
    "    \n",
    "    # fit regression\n",
    "    reg = LogisticRegression().fit(X, y)\n",
    "    score = reg.score(X, y)\n",
    "#     print(score)\n",
    "    \n",
    "#     # regression betas\n",
    "#     betas = reg.coef_\n",
    "#     #print(betas)\n",
    "    \n",
    "#     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "    all_mn_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76565c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Average accuracy multinomial regression using quartiles: %f' % (sum(all_mn_scores) / len(all_mn_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fa179",
   "metadata": {},
   "source": [
    "## Correlations between confidence and indiviudal electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4d70c",
   "metadata": {},
   "source": [
    "plotting confidence median split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59346e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "participant_corrs = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "    print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    print('\\033[0m')\n",
    "    \n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "    \n",
    "    # independent variables\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    # crop epochs for correlation computation\n",
    "    X = epochs.get_data()\n",
    "    \n",
    "    # get epoch average for each channel (average across timepoints) (n_epochs, n_channels)\n",
    "    X = np.mean(X, 2)\n",
    "    # divide into even and odd trials\n",
    "    X_even = X[0::2, :]\n",
    "    X_odd = X[1::2, :]\n",
    "\n",
    "    # dependent variable as numpy array (n_epochs,)\n",
    "    y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "    # divide into odd an even trials\n",
    "    y_even = y[0::2]\n",
    "    y_odd = y[1::2]\n",
    "    \n",
    "    \n",
    "    # compute correlation of every channel with confidence\n",
    "    all_corrs = []\n",
    "    for channel in range (X.shape[1]):\n",
    "        x = X[:, channel]\n",
    "        cor = np.corrcoef(y, x)[0, 1]\n",
    "        all_corrs.append(cor)\n",
    "    min_index = np.argmin(all_corrs)\n",
    "    \n",
    "    \n",
    "    # plot topoploy of correlations\n",
    "    print(\"Correlation of each channel with confidence \\n\\n\")\n",
    "    mne.viz.plot_topomap(data=all_corrs , pos=epochs.info, names=epochs.ch_names, show_names=True)\n",
    "    \n",
    "    \n",
    "    print(\"\\n Most negative correlation with confidence is at %s with %.3f\" %(epochs.ch_names[min_index], max(all_corrs)))\n",
    "#     sns.regplot(x=y, y=X[:, max_index], scatter_kws={'alpha':0.5})\n",
    "#     plt.xlabel('Reported confidence')\n",
    "#     plt.ylabel('Average voltage')\n",
    "#     plt.show();\n",
    "    \n",
    "    participant_corrs.append(all_corrs)\n",
    "    \n",
    "\n",
    "    # plot high vs low confidence at the channel that min correlates with confidence\n",
    "    print(\"\\n high vs low confidence at the channel that most negatively correlates with confidence\")\n",
    "    median_confidence = epochs.metadata['participant_confidence'].median()\n",
    "    low_conf_evoked = epochs['participant_confidence <= %i' % median_confidence].average()\n",
    "    high_conf_evoked = epochs['participant_confidence > %i' % median_confidence].average()\n",
    "    evokeds = dict(high_confidence=high_conf_evoked, low_confidence=low_conf_evoked)\n",
    "    mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[min_index]], invert_y=False)\n",
    "    \n",
    "#     # plot confidence quartiles at the channel that max correlates with confidence\n",
    "#     quartiles = np.percentile(y, [25,50,75]) \n",
    "#     y_quartiles = 4 - (quartiles[:,None] >=  y).sum(0)\n",
    "#     epochs.metadata['confidence_quartile'] = y_quartiles\n",
    "#     Q1_epochs = epochs['confidence_quartile == %i' % 1].average()\n",
    "#     Q2_epochs = epochs['confidence_quartile == %i' % 2].average()\n",
    "#     Q3_epochs = epochs['confidence_quartile == %i' % 3].average()\n",
    "#     Q4_epochs = epochs['confidence_quartile == %i' % 4].average()\n",
    "#     evokeds = dict(highest_confidence=Q4_epochs, high_confidence=Q3_epochs, low_confidence=Q2_epochs, lowest_confidence=Q1_epochs)\n",
    "#     mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "    \n",
    "#     # downsampled plot\n",
    "#     epochs = epochs.decimate(20)\n",
    "#     Q1_epochs = epochs['confidence_quartile == %i' % 1].average()\n",
    "#     Q2_epochs = epochs['confidence_quartile == %i' % 2].average()\n",
    "#     Q3_epochs = epochs['confidence_quartile == %i' % 3].average()\n",
    "#     Q4_epochs = epochs['confidence_quartile == %i' % 4].average()\n",
    "#     evokeds = dict(highest_confidence=Q4_epochs, high_confidence=Q3_epochs, low_confidence=Q2_epochs, lowest_confidence=Q1_epochs)\n",
    "#     mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # plot high vs low confidence z-scored by partner at the channel that max correlates with confidence\n",
    "#     zscore = lambda x: (x - x.mean()) / x.std()\n",
    "#     epochs.metadata['confidence_z_by_partner'] = epochs.metadata['participant_confidence'].groupby(epochs.metadata['partner']).transform(zscore)\n",
    "#     median_confidence = epochs.metadata['confidence_z_by_partner'].median()\n",
    "    \n",
    "#     low_conf_epochs = epochs['confidence_z_by_partner <= %i' % median_confidence]\n",
    "#     high_conf_epochs = epochs['confidence_z_by_partner > %i' % median_confidence]\n",
    "#     low_conf_evoked = low_conf_epochs.average()\n",
    "#     high_conf_evoked = high_conf_epochs.average()\n",
    "#     evokeds = dict(high_confidence=high_conf_evoked, low_confidence=low_conf_evoked)\n",
    "#     mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "    \n",
    "    \n",
    "#     # compute correlation of every channel with confidence in even trials\n",
    "#     all_corrs_even = []\n",
    "#     for channel in range (X_even.shape[1]):\n",
    "#         x_even = X_even[:, channel]\n",
    "#         cor_even = np.corrcoef(y_even, x_even)[0, 1]\n",
    "#         all_corrs_even.append(cor_even)\n",
    "#     max_index_even = np.argmax(all_corrs_even)\n",
    "#     print(\"\\n\\n For even trials, maximum correlation with confidence is at %s with %.3f\" %(epochs.ch_names[max_index_even], max(all_corrs_even)))\n",
    "#     cor_odd = np.correlate(y_odd, X_odd[:, max_index_even])\n",
    "#     print(\"\\n For odd trials, correlation with confidence at %s is %.3f\" %(epochs.ch_names[max_index_even], cor_odd))\n",
    "\n",
    "    \n",
    "#     # compute correlation of every channel with confidence in odd trials\n",
    "#     all_corrs_odd = []\n",
    "#     for channel in range (X_odd.shape[1]):\n",
    "#         x_odd = X_odd[:, channel]\n",
    "#         cor_odd = np.corrcoef(y_odd, x_odd)[0, 1]\n",
    "#         all_corrs_odd.append(cor_odd)\n",
    "#     max_index_odd = np.argmax(all_corrs_odd)\n",
    "#     print(\"\\n\\n For odd trials, maximum correlation with confidence is at %s with %.3f\" %(epochs.ch_names[max_index_odd], max(all_corrs_odd)))\n",
    "#     cor_even = np.correlate(y_even, X_even[:, max_index_odd])\n",
    "#     print(\"\\n For even trials, correlation with confidence at %s is %.3f\" %(epochs.ch_names[max_index_odd], cor_even))\n",
    "\n",
    "\n",
    "participant_correlations = np.array(participant_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e2a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_participant_correlations = np.mean(participant_correlations, 0)\n",
    "average_participant_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc73f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.min(average_participant_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4c044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "print(\"\\n Average correlation of each channel with confidence across participants\")\n",
    "mne.viz.plot_topomap(data=average_participant_correlations , pos=epochs.info, names=epochs.ch_names, show_names=True)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_neg = epochs.ch_names[average_participant_correlations.argsort()[1]]\n",
    "largest_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fae4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_largest_neg = epochs.ch_names[average_participant_correlations.argsort()[2]]\n",
    "second_largest_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e764c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_largest_neg = epochs.ch_names[average_participant_correlations.argsort()[3]]\n",
    "third_largest_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ec937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_largest_neg = epochs.ch_names[average_participant_correlations.argsort()[4]]\n",
    "fourth_largest_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_largest_neg = epochs.ch_names[average_participant_correlations.argsort()[5]]\n",
    "fifth_largest_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51dac89",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630abb6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "participant_files = []\n",
    "\n",
    "for session in sessions:\n",
    "    for subject in participant_numbers:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "\n",
    "remerge = False\n",
    "if remerge:\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    epochs.save('mergedData/response_epoch_mastoids-epo.fif', overwrite=True)\n",
    "else:\n",
    "    epochs = mne.read_epochs('mergedData/response_epoch_mastoids-epo.fif')\n",
    "\n",
    "data = epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "# zscore by participant\n",
    "#epochs.metadata['confidence_z_by_participant'] = epochs.metadata['participant_confidence'].groupby(epochs.metadata['participant']).transform(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategic_epochs = epochs['condition == \"s\"']\n",
    "strategic_epochs.metadata['confidence_z_by_participant'] = strategic_epochs.metadata['participant_confidence'].groupby(strategic_epochs.metadata['participant']).transform(zscore)\n",
    "\n",
    "nonstrategic_epochs = epochs['condition == \"ns\"']\n",
    "nonstrategic_epochs.metadata['confidence_z_by_participant'] = nonstrategic_epochs.metadata['participant_confidence'].groupby(nonstrategic_epochs.metadata['participant']).transform(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417311bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\"Pz\", \"CPz\", \"POz\", \"P1\", \"P2\"]\n",
    "#roi_1 = [\"Pz\", \"CPz\", \"POz\", \"P1\", \"P2\", \"Cz\"]\n",
    "#roi_2 = [\"P3\", \"CP3\", \"P1\", \"CP1\", \"Pz\", \"CPz\", \"P2\", \"CP2\", \"P4\", \"CP4\"]\n",
    "highest_corrs_neg = [largest_neg, second_largest_neg, third_largest_neg, fourth_largest_neg, fifth_largest_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e4a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## STRATEGIC\n",
    "\n",
    "grand_average_underconf_high_conf = []\n",
    "grand_average_underconf_low_conf = []\n",
    "grand_average_overconf_high_conf = []\n",
    "grand_average_overconf_low_conf = []\n",
    "\n",
    "underconf_epochs = strategic_epochs['partner == \"underconfident\"']\n",
    "underconf_high_conf_epochs = underconf_epochs['confidence_z_by_participant > 0']\n",
    "underconf_low_conf_epochs = underconf_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "overconf_epochs = strategic_epochs['partner == \"overconfident\"']\n",
    "overconf_high_conf_epochs = overconf_epochs['confidence_z_by_participant > 0']\n",
    "overconf_low_conf_epochs = overconf_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "\n",
    "for subject in participant_numbers:    \n",
    "    grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "    grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "    grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    \n",
    "\n",
    "# # plot with 95% confidence intervals\n",
    "# evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "#                underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "#                overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "#                overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "# mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, invert_y=False)\n",
    "\n",
    "\n",
    "\n",
    "# plot without CIs\n",
    "grand_average_underconf_high_conf = mne.grand_average(grand_average_underconf_high_conf)\n",
    "grand_average_underconf_low_conf = mne.grand_average(grand_average_underconf_low_conf)\n",
    "grand_average_overconf_high_conf = mne.grand_average(grand_average_overconf_high_conf)\n",
    "grand_average_overconf_low_conf = mne.grand_average(grand_average_overconf_low_conf)\n",
    "\n",
    "evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "               underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "               overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "               overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "print(\"\\n\\n strategic task, Pz\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=['Pz'], invert_y=False)\n",
    "print(\"\\n\\n strategic task, roi\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=roi, invert_y=False)\n",
    "print(\"\\n\\n strategic task, most negatively correlating electrodes\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=highest_corrs_neg, invert_y=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8519b84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## STRATEGIC -  z-scoring confidence by partner\n",
    "\n",
    "# grand_average_underconf_high_conf = []\n",
    "# grand_average_underconf_low_conf = []\n",
    "# grand_average_overconf_high_conf = []\n",
    "# grand_average_overconf_low_conf = []\n",
    "\n",
    "# underconf_epochs = strategic_epochs['partner == \"underconfident\"']\n",
    "# underconf_epochs.metadata['confidence_z_by_participant_by_partner'] = underconf_epochs.metadata['participant_confidence'].groupby(underconf_epochs.metadata['participant']).transform(zscore)\n",
    "# underconf_high_conf_epochs = underconf_epochs['confidence_z_by_participant_by_partner > 0']\n",
    "# underconf_low_conf_epochs = underconf_epochs['confidence_z_by_participant_by_partner <= 0']\n",
    "\n",
    "# overconf_epochs = strategic_epochs['partner == \"overconfident\"']\n",
    "# overconf_epochs.metadata['confidence_z_by_participant_by_partner'] = overconf_epochs.metadata['participant_confidence'].groupby(overconf_epochs.metadata['participant']).transform(zscore)\n",
    "# overconf_epochs = underconf_epochs['confidence_z_by_participant_by_partner > 0']\n",
    "# overconf_epochs = underconf_epochs['confidence_z_by_participant_by_partner <= 0']\n",
    "\n",
    "\n",
    "# for subject in participant_numbers:    \n",
    "#     grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "#     grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "#     grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "#     grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    \n",
    "\n",
    "# grand_average_underconf_high_conf = mne.grand_average(grand_average_underconf_high_conf)\n",
    "# grand_average_underconf_low_conf = mne.grand_average(grand_average_underconf_low_conf)\n",
    "# grand_average_overconf_high_conf = mne.grand_average(grand_average_overconf_high_conf)\n",
    "# grand_average_overconf_low_conf = mne.grand_average(grand_average_overconf_low_conf)\n",
    "\n",
    "# evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "#                underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "#                overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "#                overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "# mne.viz.plot_compare_evokeds(evokeds, picks=['Pz'], invert_y=False)\n",
    "# mne.viz.plot_compare_evokeds(evokeds, picks=roi, invert_y=False)\n",
    "# mne.viz.plot_compare_evokeds(evokeds, picks=highest_corrs_neg, invert_y=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4a100",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NON-STRATEGIC \n",
    "\n",
    "grand_average_underconf_high_conf = []\n",
    "grand_average_underconf_low_conf = []\n",
    "grand_average_overconf_high_conf = []\n",
    "grand_average_overconf_low_conf = []\n",
    "\n",
    "underconf_epochs = nonstrategic_epochs['partner == \"underconfident\"']\n",
    "underconf_high_conf_epochs = underconf_epochs['confidence_z_by_participant > 0']\n",
    "underconf_low_conf_epochs = underconf_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "overconf_epochs = nonstrategic_epochs['partner == \"overconfident\"']\n",
    "overconf_high_conf_epochs = overconf_epochs['confidence_z_by_participant > 0']\n",
    "overconf_low_conf_epochs = overconf_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "\n",
    "for subject in participant_numbers:    \n",
    "    grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "    grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "    grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    \n",
    "\n",
    "# # plot with 95% confidence intervals\n",
    "# evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "#                underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "#                overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "#                overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "# mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False)\n",
    "\n",
    "\n",
    "\n",
    "# plot without CIs\n",
    "grand_average_underconf_high_conf = mne.grand_average(grand_average_underconf_high_conf)\n",
    "grand_average_underconf_low_conf = mne.grand_average(grand_average_underconf_low_conf)\n",
    "grand_average_overconf_high_conf = mne.grand_average(grand_average_overconf_high_conf)\n",
    "grand_average_overconf_low_conf = mne.grand_average(grand_average_overconf_low_conf)\n",
    "\n",
    "evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "               underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "               overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "               overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "print(\"\\n\\n non-strategic task, Pz\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=['Pz'], invert_y=False)\n",
    "print(\"\\n\\n non-strategic task, roi\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=roi, invert_y=False)\n",
    "print(\"\\n\\n non-strategic task, most negatively correlating electrodes\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=highest_corrs_neg, invert_y=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd152b2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## NON-STRATEGIC -  z-scoring confidence by partner\n",
    "\n",
    "# grand_average_underconf_high_conf = []\n",
    "# grand_average_underconf_low_conf = []\n",
    "# grand_average_overconf_high_conf = []\n",
    "# grand_average_overconf_low_conf = []\n",
    "\n",
    "# underconf_epochs = nonstrategic_epochs['partner == \"underconfident\"']\n",
    "# underconf_epochs.metadata['confidence_z_by_participant_by_partner'] = underconf_epochs.metadata['participant_confidence'].groupby(underconf_epochs.metadata['participant']).transform(zscore)\n",
    "# underconf_high_conf_epochs = underconf_epochs['confidence_z_by_participant_by_partner > 0']\n",
    "# underconf_low_conf_epochs = underconf_epochs['confidence_z_by_participant_by_partner <= 0']\n",
    "\n",
    "# overconf_epochs = nonstrategic_epochs['partner == \"overconfident\"']\n",
    "# overconf_epochs.metadata['confidence_z_by_participant_by_partner'] = overconf_epochs.metadata['participant_confidence'].groupby(overconf_epochs.metadata['participant']).transform(zscore)\n",
    "# overconf_epochs = underconf_epochs['confidence_z_by_participant_by_partner > 0']\n",
    "# overconf_epochs = underconf_epochs['confidence_z_by_participant_by_partner <= 0']\n",
    "\n",
    "\n",
    "# for subject in participant_numbers:    \n",
    "#     grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "#     grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "#     grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "#     grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    \n",
    "\n",
    "# grand_average_underconf_high_conf = mne.grand_average(grand_average_underconf_high_conf)\n",
    "# grand_average_underconf_low_conf = mne.grand_average(grand_average_underconf_low_conf)\n",
    "# grand_average_overconf_high_conf = mne.grand_average(grand_average_overconf_high_conf)\n",
    "# grand_average_overconf_low_conf = mne.grand_average(grand_average_overconf_low_conf)\n",
    "\n",
    "# evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "#                underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "#                overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "#                overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "# mne.viz.plot_compare_evokeds(evokeds, picks=['Pz'], invert_y=False)\n",
    "# mne.viz.plot_compare_evokeds(evokeds, picks=roi, invert_y=False)\n",
    "# mne.viz.plot_compare_evokeds(evokeds, picks=highest_corrs_neg, invert_y=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adf292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5b1db07",
   "metadata": {},
   "source": [
    "### Compute average voltage accross roi for every trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5199903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b07297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roi_indices = [30, 31, 29, 19, 56]\n",
    "roi_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42410b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%capture\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for subject in participant_numbers:   \n",
    "\n",
    "    # merge response epochs from both sessions for the participant\n",
    "    participant_files = []\n",
    "    for session in sessions:\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "    epoch = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    \n",
    "    # crop epochs\n",
    "    epoch = epoch.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "\n",
    "    # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "    X = epoch.get_data()\n",
    "    \n",
    "    Pz = X[:, roi_indices[0], :]\n",
    "    Pz_trial_average = np.mean(Pz, 1) # average across timepoints\n",
    "    \n",
    "    CPz = X[:, roi_indices[1], :]\n",
    "    CPz_trial_average = np.mean(CPz, 1)\n",
    "    \n",
    "    POz = X[:, roi_indices[2], :]\n",
    "    POz_trial_average = np.mean(POz, 1)\n",
    "    \n",
    "    P1 = X[:, roi_indices[3], :]\n",
    "    P1_trial_average = np.mean(P1, 1)\n",
    "    \n",
    "    P2 = X[:, roi_indices[4], :]\n",
    "    P2_trial_average = np.mean(P2, 1)\n",
    "    \n",
    "    Pe_amplitude=[]\n",
    "    for i in range(Pz_trial_average.shape[0]):\n",
    "        trial_amplitude = (Pz_trial_average[i]+CPz_trial_average[i]+POz_trial_average[i]+P1_trial_average[i]+P2_trial_average[i])/5\n",
    "        Pe_amplitude.append(trial_amplitude)\n",
    "        \n",
    "    epoch.metadata[\"Pe_amplitude\"] = Pe_amplitude\n",
    "    \n",
    "    y = epoch.metadata['participant_confidence'].to_numpy()\n",
    "    x = epoch.metadata['Pe_amplitude'].to_numpy()\n",
    "    a = pearsonr(x, y)[0]\n",
    "    print(a)\n",
    "    correlations.append(a)\n",
    "    \n",
    "    \n",
    "    df = epoch.metadata\n",
    "    df.to_csv('metadata_Pe/participant_%i.csv' %subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55502811",
   "metadata": {},
   "source": [
    "### Compare activity between partner conditions at each electrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff7085",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALL DATA\n",
    "\n",
    "underconf_epochs = epochs['partner == \"underconfident\"']\n",
    "overconf_epochs = epochs['partner == \"overconfident\"']\n",
    "\n",
    "underconf_evokeds = []\n",
    "overconf_evokeds = []\n",
    "\n",
    "for subject in participant_numbers:    \n",
    "    underconf_evokeds.append(underconf_epochs['participant == %i' % subject].average())\n",
    "    overconf_evokeds.append(overconf_epochs['participant == %i' % subject].average())\n",
    "\n",
    "# comapre ERPs\n",
    "evokeds = dict(overconf_partner=overconf_evokeds, underconf_partner=underconf_evokeds)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title='Pz, across conditions')\n",
    "\n",
    "# difference wave\n",
    "diff_waves = []\n",
    "for subject in participant_numbers:\n",
    "    diff_waves.append(mne.combine_evoked([overconf_epochs['participant == %i' % subject].average(), underconf_epochs['participant == %i' % subject].average()],\n",
    "                                          weights=[1, -1]))\n",
    "evokeds = dict(difference_wave=diff_waves)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title=\"Pz, difference wave (overconfident-underconfident partner)\")\n",
    "\n",
    "\n",
    "# average topomap for difference wave\n",
    "mne.viz.plot_evoked_topomap(mne.grand_average(diff_waves), \n",
    "                            title=\"overconfident-underconfident partner, 0.100-0.700s\",\n",
    "                            times=.4, average=0.6,  # .20 to .80\n",
    "                            size=3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# t-test\n",
    "print('\\033[1m' + \"\\n \\n - A priori t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"testing for difference (overconfident-underconfident partner) to be different from zero at Pz\")\n",
    "# test if ERP effect is significant \n",
    "# t-test between pair of conditions on the ERP data averaged over time period of interest and at one or a few electrodes of interest\n",
    "# ttest_1samp() function from SciPy’s stats module: tests whether a set of values are different from zero\n",
    "# compute the average over the 200–800 ms time window, for each participant, at electrode Pz, and store these in a NumPy array on which we then perform the t test\n",
    "evoked_data = np.array([np.mean(e.get_data(picks='Pz', tmin=.100, tmax=.700), axis=1) for e in diff_waves])\n",
    "t, pval = stats.ttest_1samp(evoked_data, 0)\n",
    "print('\\n Difference t = ', str(round(t[0], 2)), 'p = ', str(round(pval[0], 4)))\n",
    "\n",
    "\n",
    "# permutation t-test\n",
    "print('\\033[1m' + \"\\n \\n - Permutation t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"Perform t-test at every electrode - using permutation t test to control for multiple comparisons\")\n",
    "evoked_data = np.array([np.mean(e.get_data(tmin=.200, tmax=.400), axis=1) for e in diff_waves]) # use all channels\n",
    "n_permutations = 50000\n",
    "T0, p_values, H0 = permutation_t_test(evoked_data, n_permutations, tail=-1)\n",
    "# print(\"t-values for each channel\")\n",
    "# T0\n",
    "# print(\"p-values for each channel (corrected for mulptiple comparions)\")\n",
    "# p_values\n",
    "\n",
    "print(\"t-value distribution under the H0 (from the permutations)\")\n",
    "sns.displot(data=H0)\n",
    "plt.show()\n",
    "\n",
    "print(\"top 5% of t-values\")\n",
    "thresh = round(n_permutations * .05)\n",
    "split = np.argpartition(H0, -thresh)[-thresh:]\n",
    "t_thresh = sorted(H0[split])[0]\n",
    "\n",
    "sns.displot(data=H0[split])\n",
    "plt.ylim(0, 1600)\n",
    "plt.show()\n",
    "\n",
    "print(\"t threshold\")\n",
    "print(t_thresh)\n",
    "\n",
    "print(\"\\n which electrodes have t-values above threshold\")\n",
    "T0 < -t_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a175bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# STRATEGIC\n",
    "\n",
    "underconf_epochs = strategic_epochs['partner == \"underconfident\"']\n",
    "overconf_epochs = strategic_epochs['partner == \"overconfident\"']\n",
    "\n",
    "underconf_evokeds = []\n",
    "overconf_evokeds = []\n",
    "\n",
    "for subject in participant_numbers:    \n",
    "    underconf_evokeds.append(underconf_epochs['participant == %i' % subject].average())\n",
    "    overconf_evokeds.append(overconf_epochs['participant == %i' % subject].average())\n",
    "\n",
    "# comapre ERPs\n",
    "evokeds = dict(overconf_partner=overconf_evokeds, underconf_partner=underconf_evokeds)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title='Pz, strategic condition')\n",
    "\n",
    "# difference wave\n",
    "diff_waves = []\n",
    "for subject in participant_numbers:\n",
    "    diff_waves.append(mne.combine_evoked([overconf_epochs['participant == %i' % subject].average(), underconf_epochs['participant == %i' % subject].average()],\n",
    "                                          weights=[1, -1]))\n",
    "evokeds = dict(difference_wave=diff_waves)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title=\"Pz, difference wave (overconfident-underconfident partner)\")\n",
    "\n",
    "\n",
    "# average topomap for difference wave\n",
    "mne.viz.plot_evoked_topomap(mne.grand_average(diff_waves), \n",
    "                            title=\"overconfident-underconfident partner, 0.100-0.700s\",\n",
    "                            times=.4, average=0.6,  # .20 to .80\n",
    "                            size=3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# t-test\n",
    "print('\\033[1m' + \"\\n \\n - A priori t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"testing for difference (overconfident-underconfident partner) to be different from zero at Pz\")\n",
    "# test if ERP effect is significant \n",
    "# t-test between pair of conditions on the ERP data averaged over time period of interest and at one or a few electrodes of interest\n",
    "# ttest_1samp() function from SciPy’s stats module: tests whether a set of values are different from zero\n",
    "# compute the average over the 200–800 ms time window, for each participant, at electrode Pz, and store these in a NumPy array on which we then perform the t test\n",
    "evoked_data = np.array([np.mean(e.get_data(picks='Pz', tmin=.100, tmax=.700), axis=1) for e in diff_waves])\n",
    "t, pval = stats.ttest_1samp(evoked_data, 0)\n",
    "print('\\n Difference t = ', str(round(t[0], 2)), 'p = ', str(round(pval[0], 4)))\n",
    "\n",
    "\n",
    "# permutation t-test\n",
    "print('\\033[1m' + \"\\n \\n - Permutation t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"Perform t-test at every electrode - using permutation t test to control for multiple comparisons\")\n",
    "evoked_data = np.array([np.mean(e.get_data(tmin=.200, tmax=.400), axis=1) for e in diff_waves]) # use all channels\n",
    "n_permutations = 50000\n",
    "T0, p_values, H0 = permutation_t_test(evoked_data, n_permutations, tail=-1)\n",
    "# print(\"t-values for each channel\")\n",
    "# T0\n",
    "# print(\"p-values for each channel (corrected for mulptiple comparions)\")\n",
    "# p_values\n",
    "\n",
    "print(\"t-value distribution under the H0 (from the permutations)\")\n",
    "sns.displot(data=H0)\n",
    "plt.show()\n",
    "\n",
    "print(\"top 5% of t-values\")\n",
    "thresh = round(n_permutations * .05)\n",
    "split = np.argpartition(H0, -thresh)[-thresh:]\n",
    "t_thresh = sorted(H0[split])[0]\n",
    "\n",
    "sns.displot(data=H0[split])\n",
    "plt.ylim(0, 1600)\n",
    "plt.show()\n",
    "\n",
    "print(\"t threshold\")\n",
    "print(t_thresh)\n",
    "\n",
    "print(\"\\n which electrodes have t-values above threshold\")\n",
    "T0 < -t_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58953ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NON-STRATEGIC\n",
    "\n",
    "underconf_epochs = nonstrategic_epochs['partner == \"underconfident\"']\n",
    "overconf_epochs = nonstrategic_epochs['partner == \"overconfident\"']\n",
    "\n",
    "underconf_evokeds = []\n",
    "overconf_evokeds = []\n",
    "\n",
    "for subject in participant_numbers:    \n",
    "    underconf_evokeds.append(underconf_epochs['participant == %i' % subject].average())\n",
    "    overconf_evokeds.append(overconf_epochs['participant == %i' % subject].average())\n",
    "\n",
    "# comapre ERPs\n",
    "evokeds = dict(overconf_partner=overconf_evokeds, underconf_partner=underconf_evokeds)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title='Pz, non-strategic condition')\n",
    "\n",
    "# difference wave\n",
    "diff_waves = []\n",
    "for subject in participant_numbers:\n",
    "    diff_waves.append(mne.combine_evoked([overconf_epochs['participant == %i' % subject].average(), underconf_epochs['participant == %i' % subject].average()],\n",
    "                                          weights=[1, -1]))\n",
    "evokeds = dict(difference_wave=diff_waves)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title=\"Pz, difference wave (overconfident-underconfident partner)\")\n",
    "\n",
    "\n",
    "# average topomap for difference wave\n",
    "mne.viz.plot_evoked_topomap(mne.grand_average(diff_waves), \n",
    "                            title=\"overconfident-underconfident partner, 0.100-0.700s\",\n",
    "                            times=.4, average=0.6,  # .20 to .80\n",
    "                            size=3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# t-test\n",
    "print('\\033[1m' + \"\\n \\n - A priori t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"testing for difference (overconfident-underconfident partner) to be different from zero at Pz\")\n",
    "# test if ERP effect is significant \n",
    "# t-test between pair of conditions on the ERP data averaged over time period of interest and at one or a few electrodes of interest\n",
    "# ttest_1samp() function from SciPy’s stats module: tests whether a set of values are different from zero\n",
    "# compute the average over the 200–800 ms time window, for each participant, at electrode Pz, and store these in a NumPy array on which we then perform the t test\n",
    "evoked_data = np.array([np.mean(e.get_data(picks='Pz', tmin=.100, tmax=.700), axis=1) for e in diff_waves])\n",
    "t, pval = stats.ttest_1samp(evoked_data, 0)\n",
    "print('\\n Difference t = ', str(round(t[0], 2)), 'p = ', str(round(pval[0], 4)))\n",
    "\n",
    "\n",
    "# permutation t-test\n",
    "print('\\033[1m' + \"\\n \\n - Permutation t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"Perform t-test at every electrode - using permutation t test to control for multiple comparisons\")\n",
    "evoked_data = np.array([np.mean(e.get_data(tmin=.200, tmax=.400), axis=1) for e in diff_waves]) # use all channels\n",
    "n_permutations = 50000\n",
    "T0, p_values, H0 = permutation_t_test(evoked_data, n_permutations, tail=-1)\n",
    "# print(\"t-values for each channel\")\n",
    "# T0\n",
    "# print(\"p-values for each channel (corrected for mulptiple comparions)\")\n",
    "# p_values\n",
    "\n",
    "print(\"t-value distribution under the H0 (from the permutations)\")\n",
    "sns.displot(data=H0)\n",
    "plt.show()\n",
    "\n",
    "print(\"top 5% of t-values\")\n",
    "thresh = round(n_permutations * .05)\n",
    "split = np.argpartition(H0, -thresh)[-thresh:]\n",
    "t_thresh = sorted(H0[split])[0]\n",
    "\n",
    "sns.displot(data=H0[split])\n",
    "plt.ylim(0, 1600)\n",
    "plt.show()\n",
    "\n",
    "print(\"t threshold\")\n",
    "print(t_thresh)\n",
    "\n",
    "print(\"\\n which electrodes have t-values above threshold\")\n",
    "T0 < -t_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701c304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1a2323",
   "metadata": {},
   "source": [
    "### Compare activity for correct/incorrect responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b2a83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALL DATA\n",
    "\n",
    "correct_epochs = epochs['participant_correct == True']\n",
    "incorrect_epochs = epochs['participant_correct == False']\n",
    "\n",
    "correct_evokeds = []\n",
    "incorrect_evokeds = []\n",
    "\n",
    "for subject in participant_numbers:    \n",
    "    correct_evokeds.append(correct_epochs['participant == %i' % subject].average())\n",
    "    incorrect_evokeds.append(incorrect_epochs['participant == %i' % subject].average())\n",
    "\n",
    "# comapre ERPs\n",
    "evokeds = dict(correct=correct_evokeds, incorrect=incorrect_evokeds)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title='Pz, across conditions')\n",
    "\n",
    "# difference wave\n",
    "diff_waves = []\n",
    "for subject in participant_numbers:\n",
    "    diff_waves.append(mne.combine_evoked([correct_epochs['participant == %i' % subject].average(), incorrect_epochs['participant == %i' % subject].average()],\n",
    "                                          weights=[-1, 1]))\n",
    "evokeds = dict(difference_wave=diff_waves)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False, title=\"Pz, difference wave (incorrect-correct)\")\n",
    "\n",
    "\n",
    "# average topomap for difference wave\n",
    "mne.viz.plot_evoked_topomap(mne.grand_average(diff_waves), \n",
    "                            title=\"correct-incorrect, 0.100-0.700s\",\n",
    "                            times=.4, average=0.6,  # .20 to .80\n",
    "                            size=3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# t-test\n",
    "print('\\033[1m' + \"\\n \\n - A priori t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"testing for difference (correct-incorrect) to be different from zero at Pz\")\n",
    "# test if ERP effect is significant \n",
    "# t-test between pair of conditions on the ERP data averaged over time period of interest and at one or a few electrodes of interest\n",
    "# ttest_1samp() function from SciPy’s stats module: tests whether a set of values are different from zero\n",
    "# compute the average over the 200–800 ms time window, for each participant, at electrode Pz, and store these in a NumPy array on which we then perform the t test\n",
    "evoked_data = np.array([np.mean(e.get_data(picks='Pz', tmin=.100, tmax=.700), axis=1) for e in diff_waves])\n",
    "t, pval = stats.ttest_1samp(evoked_data, 0)\n",
    "print('\\n Difference t = ', str(round(t[0], 2)), 'p = ', str(round(pval[0], 4)))\n",
    "\n",
    "\n",
    "# permutation t-test\n",
    "print('\\033[1m' + \"\\n \\n - Permutation t-test -\")\n",
    "print('\\033[0m')\n",
    "print(\"Perform t-test at every electrode - using permutation t test to control for multiple comparisons\")\n",
    "evoked_data = np.array([np.mean(e.get_data(tmin=.200, tmax=.400), axis=1) for e in diff_waves]) # use all channels\n",
    "n_permutations = 50000\n",
    "T0, p_values, H0 = permutation_t_test(evoked_data, n_permutations, tail=-1)\n",
    "# print(\"t-values for each channel\")\n",
    "# T0\n",
    "# print(\"p-values for each channel (corrected for mulptiple comparions)\")\n",
    "# p_values\n",
    "\n",
    "print(\"t-value distribution under the H0 (from the permutations)\")\n",
    "sns.displot(data=H0)\n",
    "plt.show()\n",
    "\n",
    "print(\"top 5% of t-values\")\n",
    "thresh = round(n_permutations * .05)\n",
    "split = np.argpartition(H0, -thresh)[-thresh:]\n",
    "t_thresh = sorted(H0[split])[0]\n",
    "\n",
    "sns.displot(data=H0[split])\n",
    "plt.ylim(0, 1600)\n",
    "plt.show()\n",
    "\n",
    "print(\"t threshold\")\n",
    "print(t_thresh)\n",
    "\n",
    "print(\"\\n which electrodes have t-values above threshold\")\n",
    "T0 < -t_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3072f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691d461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9a795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2520507e",
   "metadata": {},
   "source": [
    "### Understanding how to get the sensor projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 4],[2, 5], [3, 6]])\n",
    "y = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04817e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, err, _, __ = np.linalg.lstsq(x, y, rcond=None)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf5534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# should give [1, 2.2857] (jasmine just uses a = y_pred \\ X_test)\n",
    "a = y.T@x * ( 1/ (y.T@y) )\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
