{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0c8211",
   "metadata": {},
   "source": [
    "Run this notebook to\n",
    "\n",
    "- xx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f78124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy above >= 62% in non-strategic task: 7, 19, 1, 11, 20, 17, 18, 9, 6, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a634423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy above >= 62% in strategic task: 1, 2, 19, 18, 11, 3, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f55c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0a054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb70ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ebb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy above >= 64% in non-strategic task: 7, 19, 1, 11, 20, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy above >= 64% in strategic task: 1, 2, 19, 18, 11, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa373f",
   "metadata": {},
   "source": [
    "## Import stuff and set parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72988d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(on) {\n",
       "const e=$( \"<a>Setup failed</a>\" );\n",
       "const ns=\"js_jupyter_suppress_warnings\";\n",
       "var cssrules=$(\"#\"+ns);\n",
       "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
       "e.click(function() {\n",
       "    var s='Showing';  \n",
       "    cssrules.empty()\n",
       "    if(on) {\n",
       "        s='Hiding';\n",
       "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
       "    }\n",
       "    e.text(s+' warnings (click to toggle)');\n",
       "    on=!on;\n",
       "}).click();\n",
       "$(element).append(e);\n",
       "})(true);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e09e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr \n",
    "from itertools import chain, zip_longest\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import mne\n",
    "from mne.stats import permutation_t_test\n",
    "mne.set_log_level('warning') \n",
    "\n",
    "#%matplotlib qt\n",
    "%matplotlib inline\n",
    "\n",
    "input_dir = 'TaskstimulusEpochsMastoids'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4dd4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subj_eeg(path, file, downsample=None):\n",
    "    fp = os.path.join(path, '%s-epo.fif' % file)\n",
    "    print('>>> Loading %s' % fp)\n",
    "    epochs = mne.read_epochs(fp, preload=True)\n",
    "    if downsample is not None:\n",
    "        epochs = epochs.resample(downsample)\n",
    "    return epochs\n",
    "\n",
    "def load_all_eeg(path, files, downsample=None):\n",
    "    subject_epochs = [load_subj_eeg(path, file, downsample=downsample) for file in files]\n",
    "    epochs = mne.epochs.concatenate_epochs(subject_epochs)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02287825",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21] # 14 was excluded due to noise\n",
    "\n",
    "sessions = [1, 2]\n",
    "\n",
    "partners = ['\"overconfident\"', '\"underconfident\"']\n",
    "\n",
    "epoch_start = 0.25\n",
    "epoch_end = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9be271",
   "metadata": {},
   "source": [
    "## Correlation reaction time and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c535ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "strategic_correlations = []\n",
    "non_strategic_correlations = []\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "        \n",
    "    \n",
    "    for subject in participant_numbers: \n",
    "        participant_files = []\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "        epochs = load_subj_eeg(path='%s/' % input_dir, file=participant_files[0])\n",
    "\n",
    "        y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "        x = epochs.metadata['decision_rt'].to_numpy()\n",
    "\n",
    "        a = pearsonr(x, y)[0]\n",
    "        print(a)\n",
    "\n",
    "        condition = epochs.metadata['condition'].unique()\n",
    "        if condition == \"s\":\n",
    "            strategic_correlations.append(a)\n",
    "        if condition == \"ns\":\n",
    "            non_strategic_correlations.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ae657e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategic average correlation confidence and reaction time: -0.338381\n",
      "\n",
      "Non-strategic average correlation confidence and reaction time: -0.250031\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\nStrategic average correlation confidence and reaction time: %f' % (sum(strategic_correlations) / len(strategic_correlations),))\n",
    "\n",
    "\n",
    "print('\\nNon-strategic average correlation confidence and reaction time: %f' % (sum(non_strategic_correlations) / len(non_strategic_correlations),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b374f",
   "metadata": {},
   "source": [
    "## Linear regression predicting confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce76ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_lr_scores = []\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "        \n",
    "\n",
    "    for subject in participant_numbers:   \n",
    "    #     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    #     print('\\033[0m')\n",
    "\n",
    "        # merge stimulus epochs from both sessions for the participant - TREATING EVERYONE INDIVIDUALLY\n",
    "        participant_files = []\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "        epochs = load_subj_eeg(path='%s/' % input_dir, file=participant_files[0])\n",
    "\n",
    "        # crop epochs\n",
    "        epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "\n",
    "        # independent variables\n",
    "        # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        n_epochs = X.shape[0]\n",
    "        n_channels = X.shape[1]\n",
    "        n_timepoints = X.shape[2]\n",
    "\n",
    "        # collapse across trials and timepoints\n",
    "        X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "        X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "        # dependent variable as numpy array (n_epochs,)\n",
    "        y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "        # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "        y = np.repeat(y, n_timepoints)\n",
    "\n",
    "        # fit regression\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        score = reg.score(X, y)\n",
    "    #     print(score)\n",
    "\n",
    "    #     # regression betas\n",
    "    #     betas = reg.coef_\n",
    "    #     #print(betas)\n",
    "\n",
    "    #     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "        all_lr_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d638a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R-squared linear regression: 0.140731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Average R-squared linear regression: %f' % (sum(all_lr_scores) / len(all_lr_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237e10a",
   "metadata": {},
   "source": [
    "## Cross-validation random split across timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c27e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_cv_scores = []\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "\n",
    "    for subject in participant_numbers:   \n",
    "    #     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    #     print('\\033[0m')\n",
    "\n",
    "        # merge stimulus epochs from both sessions for the participant - TREATING EVERYONE INDIVIDUALLY\n",
    "        participant_files = []\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "        epochs = load_subj_eeg(path='%s/' % input_dir, file=participant_files[0])\n",
    "    \n",
    "        # crop epochs\n",
    "        epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "\n",
    "\n",
    "\n",
    "        ###### INDEPENDENT VARIABLES\n",
    "        # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "        X = epochs.get_data()\n",
    "        n_channels = X.shape[1]\n",
    "        n_timepoints = X.shape[2]\n",
    "        X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "\n",
    "\n",
    "\n",
    "        ###### DEPENDENT VARIABLES\n",
    "        ## this version is splitting data randomly by timepoints - topoplots etc look v similar\n",
    "        # 50% training data, 50% test data\n",
    "        X = X.reshape(-1, n_channels)\n",
    "        y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "        y = np.repeat(y, n_timepoints)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "        ###### TRAIN CLASSIFIER\n",
    "        regressor = LinearRegression()  \n",
    "        regressor.fit(X_train, y_train) #training the algorithm\n",
    "\n",
    "        # regression betas\n",
    "        betas = regressor.coef_\n",
    "\n",
    "        # predict y from X_test\n",
    "        y_pred = regressor.predict(X_test)    \n",
    "\n",
    "\n",
    "        ###### PLOT\n",
    "        # plot topoplot with beta weights\n",
    "        mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "        print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "        # plot topoplot with sensor projections\n",
    "        # get out sensor projections (see Parra et al paper)\n",
    "        y = y_pred\n",
    "        x = X_test\n",
    "        a =  y.T@x * ( 1 / (y.T@y) )\n",
    "        mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "        print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "        # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "        across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "        weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "        average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "        x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "        plt.plot(x_axis, average_weighted_erp)\n",
    "        plt.show();\n",
    "        print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")\n",
    "\n",
    "    #     # plot normal ERP over time\n",
    "    #     across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "    #     average_across_epochs_erp = across_epochs_erp.mean(axis=0) # compute the average across electrodes \n",
    "    #     x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "    #     plt.plot(x_axis, average_across_epochs_erp)\n",
    "    #     plt.show()\n",
    "    #     print(\"Average ERP (not weighted) \\n\\n\")\n",
    "\n",
    "\n",
    "        print('R-squared:', r2_score(y_test, y_pred))  \n",
    "        all_cv_scores.append(r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc2365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Average R-squared cross validation random timepoint split: %f' % (sum(all_cv_scores) / len(all_cv_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c06df",
   "metadata": {},
   "source": [
    "## Cross-validation training on odd (even) trials, testing on even (odd) trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff573b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores_test = []\n",
    "all_scores_test_trialwise = []\n",
    "all_scores_train = []\n",
    "all_scores_train_trialwise = []\n",
    "\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "\n",
    "    for subject in participant_numbers:   \n",
    "    #     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    #     print('\\033[0m')\n",
    "\n",
    "        # merge stimulus epochs from both sessions for the participant - TREATING EVERYONE INDIVIDUALLY\n",
    "        participant_files = []\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "        epochs = load_subj_eeg(path='%s/' % input_dir, file=participant_files[0])\n",
    "    \n",
    "        # crop epochs\n",
    "        epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        ####################### INDEPENDENT VARIABLE ######################### \n",
    "        ######################################################################\n",
    "\n",
    "        # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times) (600, 64, 127) (250 Hz --> every 4 ms)\n",
    "        X = epochs.get_data()\n",
    "        n_epochs = X.shape[0]\n",
    "        n_channels = X.shape[1]\n",
    "        n_timepoints = X.shape[2]\n",
    "        X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels) (600, 127, 64)\n",
    "\n",
    "        # divide into odd and even trials (3D array) (300, 127, 64)\n",
    "        X_odd_trials = X[1::2, :, :] # select every second epoch (start:stop:step, :, :)\n",
    "        X_even_trials = X[0::2, :, :] # select every other second epoch\n",
    "\n",
    "        # collapse across trials and timepoints (2D array) (triple checked this and it is correctly sorted as e1t1, e1t2, e1t3, ...)\n",
    "        X_odd_per_timepoint = X_odd_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "        X_even_per_timepoint = X_even_trials.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        ######################## DEPENDENT VARIABLE ########################## \n",
    "        ######################################################################\n",
    "\n",
    "        # dependent variable as numpy array (n_epochs,)\n",
    "        y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "\n",
    "        # divide into odd and even trials\n",
    "        y_odd_trials = y[1::2]\n",
    "        y_even_trials = y[0::2]\n",
    "\n",
    "        # make from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "        y_odd_per_timepoint = np.repeat(y_odd_trials, n_timepoints)\n",
    "        y_even_per_timepoint = np.repeat(y_even_trials, n_timepoints)\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        ####################### TRAINING ON ODD TRIALS ####################### \n",
    "        ######################################################################\n",
    "\n",
    "        print('\\033[1m' + \"\\n\\n ______ train on odd trials ______\\n\")\n",
    "        print('\\033[0m')\n",
    "\n",
    "        X_train_trials = X_odd_trials\n",
    "        X_train_per_timepoint = X_odd_per_timepoint\n",
    "\n",
    "        X_test_trials = X_even_trials\n",
    "        X_test_per_timepoint = X_even_per_timepoint\n",
    "\n",
    "\n",
    "        y_train_trials = y_odd_trials\n",
    "        y_train_per_timepoint = y_odd_per_timepoint\n",
    "\n",
    "        y_test_trials = y_even_trials\n",
    "        y_test_per_timepoint = y_even_per_timepoint\n",
    "\n",
    "\n",
    "\n",
    "        # train classifier\n",
    "        regressor = LinearRegression()  \n",
    "        regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "        betas = regressor.coef_   \n",
    "        intercept = regressor.intercept_\n",
    "\n",
    "        # predict y from X_test \n",
    "        # OPTION 1\n",
    "        # use predict function on individual timepoints\n",
    "        y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "        # then average predicted values within an epoch\n",
    "        epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "        # OPTION 2\n",
    "        # use betas and intercept on individual timepoints   \n",
    "        y_pred_per_timepoint_2 = np.sum(X_test_per_timepoint * betas, axis=1) + intercept \n",
    "        # then average predicted values within an epoch\n",
    "        epoch_y_pred_2 = np.mean(y_pred_per_timepoint_2.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "        # OPTION 3\n",
    "        # use predict function on epoch average\n",
    "        epoch_y_pred_3 = regressor.predict(np.mean(X_test_trials, axis=1)) # take mean voltage along n_times axis (n_epochs, n_times, n_channels)\n",
    "\n",
    "        # OPTION 4\n",
    "        # use betas and intercept on epoch average\n",
    "        epoch_y_pred_4 = np.sum(np.mean(X_test_trials, axis=1) * betas, axis=1) + intercept \n",
    "\n",
    "    #     # sanity check - all options give the same predicted y per epoch\n",
    "    #     print(epoch_y_pred_1[0:6])\n",
    "    #     print(epoch_y_pred_2[0:6])\n",
    "    #     print(epoch_y_pred_3[0:6])\n",
    "    #     print(epoch_y_pred_4[0:6])\n",
    "\n",
    "\n",
    "        print('\\033[1m' + \"\\n test data\")\n",
    "        print('\\033[0m')\n",
    "        # R squared for timepoints\n",
    "        print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    #     # sanity check - manual computation gives same R-squared\n",
    "    #     actual_minus_predicted = sum((y_test_per_timepoint - y_pred_per_timepoint_1)**2)\n",
    "    #     actual_minus_actual_mean = sum((y_test_per_timepoint - y_test_per_timepoint.mean())**2)\n",
    "    #     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "    #     print('R-squared timepoint data:', r2)\n",
    "\n",
    "        # R squared for trials\n",
    "        print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "    #     # sanity check - manual computation gives same R-squared\n",
    "    #     actual_minus_predicted = sum((y_test_trials - epoch_y_pred_1)**2)\n",
    "    #     actual_minus_actual_mean = sum((y_test_trials - y_test_trials.mean())**2)\n",
    "    #     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "    #     print('R-squared trial data:', r2)\n",
    "        # plot y against predicted y\n",
    "        sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "        plt.xlabel('Reported confidence')\n",
    "        plt.ylabel('Classifier prediction')\n",
    "        plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "        plt.show();\n",
    "\n",
    "\n",
    "        # R squared for training data\n",
    "        print('\\033[1m' + \"\\n train data\")\n",
    "        print('\\033[0m')\n",
    "        # R squared for timepoints\n",
    "        y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "        print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "        # R squared for trials\n",
    "        epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "        print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "        # plot y against predicted y\n",
    "        sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "        plt.xlabel('Reported confidence')\n",
    "        plt.ylabel('Classifier prediction')\n",
    "        plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "        plt.show();\n",
    "\n",
    "        all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "        all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "        all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "        all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "\n",
    "\n",
    "        ####################### PLOTTING ####################### \n",
    "\n",
    "        # plot topoplot with beta weights\n",
    "        mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "        print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "\n",
    "        # plot topoplot with sensor projections (see Parra et al paper for equation)\n",
    "        y = y_pred_per_timepoint\n",
    "        x = X_test_per_timepoint\n",
    "        a =  y.T@x * ( 1 / (y.T@y) )\n",
    "        mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "        print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "    #     # sanity check - plotting with trial data rather than timepoint data gives the same plot\n",
    "    #     y = epoch_y_pred\n",
    "    #     x = np.mean(X_test_trials, axis=1) \n",
    "    #     a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    #     mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    #     print(\"Sensor projections trial data \\n\\n\")\n",
    "\n",
    "        # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "        across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "        weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "        average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "        x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "        plt.plot(x_axis, average_weighted_erp)\n",
    "        plt.show()\n",
    "        print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        ####################### TRAINING ON EVEN TRIALS ###################### \n",
    "        ######################################################################\n",
    "\n",
    "        print('\\033[1m' + \"\\n\\n ______ train on even trials ______\\n\")\n",
    "        print('\\033[0m')\n",
    "\n",
    "        X_train_trials = X_even_trials\n",
    "        X_train_per_timepoint = X_even_per_timepoint\n",
    "\n",
    "        X_test_trials = X_odd_trials\n",
    "        X_test_per_timepoint = X_odd_per_timepoint\n",
    "\n",
    "\n",
    "        y_train_trials = y_even_trials\n",
    "        y_train_per_timepoint = y_even_per_timepoint\n",
    "\n",
    "        y_test_trials = y_odd_trials\n",
    "        y_test_per_timepoint = y_odd_per_timepoint\n",
    "\n",
    "\n",
    "\n",
    "        # train classifier\n",
    "        regressor = LinearRegression()  \n",
    "        regressor.fit(X_train_per_timepoint, y_train_per_timepoint) #training the algorithm on every timepoint\n",
    "        betas = regressor.coef_\n",
    "        intercept = regressor.intercept_\n",
    "\n",
    "        # predict y from X_test \n",
    "        # use predict function on individual timepoints\n",
    "        y_pred_per_timepoint = regressor.predict(X_test_per_timepoint) \n",
    "        # then average predicted values within an epoch\n",
    "        epoch_y_pred = np.mean(y_pred_per_timepoint.reshape(-1, n_timepoints), axis=1)\n",
    "\n",
    "        print('\\033[1m' + \"\\n test data\")\n",
    "        print('\\033[0m')\n",
    "        # R squared for timepoints\n",
    "        print('R-squared timepoint data:', r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "    #     # sanity check - manual computation gives same R-squared\n",
    "    #     actual_minus_predicted = sum((y_test_per_timepoint - y_pred_per_timepoint_1)**2)\n",
    "    #     actual_minus_actual_mean = sum((y_test_per_timepoint - y_test_per_timepoint.mean())**2)\n",
    "    #     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "    #     print('R-squared timepoint data:', r2)\n",
    "\n",
    "        # R squared for trials\n",
    "        print('R-squared trial data:', r2_score(y_test_trials, epoch_y_pred))\n",
    "    #     # sanity check - manual computation gives same R-squared\n",
    "    #     actual_minus_predicted = sum((y_test_trials - epoch_y_pred_1)**2)\n",
    "    #     actual_minus_actual_mean = sum((y_test_trials - y_test_trials.mean())**2)\n",
    "    #     r2 = 1 - actual_minus_predicted/actual_minus_actual_mean\n",
    "    #     print('R-squared trial data:', r2)\n",
    "        # plot y against predicted y\n",
    "        sns.regplot(x=y_test_trials, y=epoch_y_pred, scatter_kws={'alpha':0.5})\n",
    "        plt.xlabel('Reported confidence')\n",
    "        plt.ylabel('Classifier prediction')\n",
    "        plt.title('Reported trial confidence against predicted trial confidence test data')\n",
    "        plt.show();\n",
    "\n",
    "\n",
    "        # R squared for training data\n",
    "        print('\\033[1m' + \"\\n train data\")\n",
    "        print('\\033[0m')\n",
    "        # R squared for timepoints\n",
    "        y_pred_per_timepoint_training = regressor.predict(X_train_per_timepoint) \n",
    "        print('R-squared training timepoint data:', r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "        # R squared for trials\n",
    "        epoch_y_pred_train = np.mean(y_pred_per_timepoint_training.reshape(-1, n_timepoints), axis=1)\n",
    "        print('R-squared training trial data:', r2_score(y_train_trials, epoch_y_pred_train))\n",
    "        # plot y against predicted y\n",
    "        sns.regplot(x=y_train_trials, y=epoch_y_pred_train, scatter_kws={'alpha':0.5})\n",
    "        plt.xlabel('Reported confidence')\n",
    "        plt.ylabel('Classifier prediction')\n",
    "        plt.title('Reported trial confidence against predicted trial confidence train data')\n",
    "        plt.show();\n",
    "\n",
    "        all_scores_test.append(r2_score(y_test_per_timepoint, y_pred_per_timepoint))\n",
    "        all_scores_test_trialwise.append(r2_score(y_test_trials, epoch_y_pred))\n",
    "        all_scores_train.append(r2_score(y_train_per_timepoint, y_pred_per_timepoint_training))\n",
    "        all_scores_train_trialwise.append(r2_score(y_train_trials, epoch_y_pred_train))\n",
    "\n",
    "\n",
    "        ####################### PLOTTING ####################### \n",
    "\n",
    "        # plot topoplot with beta weights\n",
    "        mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "        print(\"Beta weights for each channel \\n\\n\")\n",
    "\n",
    "        # plot topoplot with sensor projections (see Parra et al paper for equation)\n",
    "        y = y_pred_per_timepoint\n",
    "        x = X_test_per_timepoint\n",
    "        a =  y.T@x * ( 1 / (y.T@y) )\n",
    "        mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "        print(\"Sensor projections \\n\\n\")\n",
    "\n",
    "    #     # shuffle ys --> gives same topoplot because my predictions are basically random and then this just plots the average voltage which happens to look like P3\n",
    "    #     y = y_pred_per_timepoint\n",
    "    #     np.random.shuffle(y)\n",
    "    #     x = X_test_per_timepoint\n",
    "    #     a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    #     mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    #     print(\"Sensor projections shuffled ys \\n\\n\")\n",
    "\n",
    "    #     # shuffle xs --> gives random topoplot\n",
    "    #     y = y_pred_per_timepoint\n",
    "    #     x = X_test_per_timepoint\n",
    "    #     def shuffle_along_axis(a, axis):\n",
    "    #         idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    #         return np.take_along_axis(a,idx,axis=axis)\n",
    "    #     x = shuffle_along_axis(x, 1)\n",
    "    #     a =  y.T@x * ( 1 / (y.T@y) )\n",
    "    #     mne.viz.plot_topomap(data=a, pos=epochs.info)\n",
    "    #     print(\"Sensor projections shuffled xs \\n\\n\")\n",
    "\n",
    "        # plot confidence signal ERP over time (use betas rather than a weights)\n",
    "        across_epochs_erp = epochs.average().get_data() # gives matrix with channels x timepoints (averaging across epochs)\n",
    "        weighted_erp = across_epochs_erp * betas.reshape((betas.size, 1)) # weigh the voltages by the channel weights\n",
    "        average_weighted_erp = weighted_erp.mean(axis=0) # compute the average across electrodes \n",
    "        x_axis = np.arange(epoch_start, epoch_end, (epoch_end-epoch_start)/(n_timepoints))\n",
    "        plt.plot(x_axis, average_weighted_erp)\n",
    "        plt.show()\n",
    "        print(\"Average voltages (across epochs) weighted by regression betas for each electrode \\n\\n\")    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaa8a6",
   "metadata": {},
   "source": [
    "R-squared timepoint wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared test timepoint-wise: %f' % (sum(all_scores_test) / len(all_scores_test),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared train timepoint-wise: %f' % (sum(all_scores_train) / len(all_scores_train),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfce5b",
   "metadata": {},
   "source": [
    "R-squared trial wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared test trialwise: %f' % (sum(all_scores_test_trialwise) / len(all_scores_test_trialwise),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\n Average R-squared train trialwise: %f' % (sum(all_scores_train_trialwise) / len(all_scores_train_trialwise),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d29dd",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92840bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "all_lg_scores = []\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "\n",
    "    for subject in participant_numbers:   \n",
    "    #     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    #     print('\\033[0m')\n",
    "\n",
    "        # merge stimulus epochs from both sessions for the participant - TREATING EVERYONE INDIVIDUALLY\n",
    "        participant_files = []\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "        epochs = load_subj_eeg(path='%s/' % input_dir, file=participant_files[0])\n",
    "        \n",
    "        \n",
    "        # crop epochs\n",
    "        epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "\n",
    "        # independent variables\n",
    "        # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        n_epochs = X.shape[0]\n",
    "        n_channels = X.shape[1]\n",
    "        n_timepoints = X.shape[2]\n",
    "\n",
    "        # collapse across trials and timepoints\n",
    "        X = X.swapaxes(1, 2) # (n_epochs, n_times, n_channels)\n",
    "        X = X.reshape(-1, n_channels) # (n_epochs x n_times, n_channels) --> n_samples, n_features\n",
    "\n",
    "        # dependent variable as numpy array (n_epochs,)\n",
    "        y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "        # make it binary\n",
    "        med_y = np.median(y)\n",
    "        y[y<=med_y] = -1\n",
    "        y[y>med_y] = 1\n",
    "\n",
    "        # extend from (n_epochs) to (n_epochs x n_timepoints, ) where confidence value stays the same\n",
    "        y = np.repeat(y, n_timepoints)\n",
    "\n",
    "        # fit regression\n",
    "        reg = LogisticRegression().fit(X, y)\n",
    "        score = reg.score(X, y)\n",
    "    #     print(score)\n",
    "\n",
    "    #     # regression betas\n",
    "    #     betas = reg.coef_\n",
    "    #     #print(betas)\n",
    "\n",
    "    #     mne.viz.plot_topomap(data=betas, pos=epochs.info)\n",
    "\n",
    "        all_lg_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Average accuracy logistic regression: %f' % (sum(all_lg_scores) / len(all_lg_scores),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fa179",
   "metadata": {},
   "source": [
    "## Correlations between confidence and indiviudal electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878da74",
   "metadata": {},
   "source": [
    "plotting confidence median split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59346e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "participant_corrs = []\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "\n",
    "    for subject in participant_numbers:   \n",
    "    #     print('\\033[1m' + \"\\n\\n----------------------- Participant %i -----------------------\\n\" % subject)\n",
    "    #     print('\\033[0m')\n",
    "\n",
    "        # merge stimulus epochs from both sessions for the participant - TREATING EVERYONE INDIVIDUALLY\n",
    "        participant_files = []\n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "        epochs = load_subj_eeg(path='%s/' % input_dir, file=participant_files[0])\n",
    "    \n",
    "        # crop epochs\n",
    "        epochs = epochs.crop(tmin=epoch_start, tmax=epoch_end, include_tmax=True)\n",
    "\n",
    "        # independent variables\n",
    "        # get epoched data as a numpy array of shape (n_epochs, n_channels, n_times)\n",
    "        # crop epochs for correlation computation\n",
    "        X = epochs.get_data()\n",
    "\n",
    "        # get epoch average for each channel (average across timepoints) (n_epochs, n_channels)\n",
    "        X = np.mean(X, 2)\n",
    "        # divide into even and odd trials\n",
    "        X_even = X[0::2, :]\n",
    "        X_odd = X[1::2, :]\n",
    "\n",
    "        # dependent variable as numpy array (n_epochs,)\n",
    "        y = epochs.metadata['participant_confidence'].to_numpy()\n",
    "        # divide into odd an even trials\n",
    "        y_even = y[0::2]\n",
    "        y_odd = y[1::2]\n",
    "\n",
    "\n",
    "        # compute correlation of every channel with confidence\n",
    "        all_corrs = []\n",
    "        for channel in range (X.shape[1]):\n",
    "            x = X[:, channel]\n",
    "            cor = np.corrcoef(y, x)[0, 1]\n",
    "            all_corrs.append(cor)\n",
    "        max_index = np.argmax(all_corrs)\n",
    "\n",
    "\n",
    "        # plot topoploy of correlations\n",
    "        print(\"Correlation of each channel with confidence \\n\\n\")\n",
    "        mne.viz.plot_topomap(data=all_corrs , pos=epochs.info, names=epochs.ch_names, show_names=True)\n",
    "\n",
    "\n",
    "        print(\"\\n Maximum correlation with confidence is at %s with %.3f\" %(epochs.ch_names[max_index], max(all_corrs)))\n",
    "    #     sns.regplot(x=y, y=X[:, max_index], scatter_kws={'alpha':0.5})\n",
    "    #     plt.xlabel('Reported confidence')\n",
    "    #     plt.ylabel('Average voltage')\n",
    "    #     plt.show();\n",
    "\n",
    "        participant_corrs.append(all_corrs)\n",
    "\n",
    "\n",
    "        # plot high vs low confidence at the channel that max correlates with confidence\n",
    "        print(\"\\n high vs low confidence at the channel that max correlates with confidence\")\n",
    "        median_confidence = epochs.metadata['participant_confidence'].median()\n",
    "        low_conf_evoked = epochs['participant_confidence <= %i' % median_confidence].average()\n",
    "        high_conf_evoked = epochs['participant_confidence > %i' % median_confidence].average()\n",
    "        evokeds = dict(high_confidence=high_conf_evoked, low_confidence=low_conf_evoked)\n",
    "        mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "\n",
    "    #     # plot confidence quartiles at the channel that max correlates with confidence\n",
    "    #     quartiles = np.percentile(y, [25,50,75]) \n",
    "    #     y_quartiles = 4 - (quartiles[:,None] >=  y).sum(0)\n",
    "    #     epochs.metadata['confidence_quartile'] = y_quartiles\n",
    "    #     Q1_epochs = epochs['confidence_quartile == %i' % 1].average()\n",
    "    #     Q2_epochs = epochs['confidence_quartile == %i' % 2].average()\n",
    "    #     Q3_epochs = epochs['confidence_quartile == %i' % 3].average()\n",
    "    #     Q4_epochs = epochs['confidence_quartile == %i' % 4].average()\n",
    "    #     evokeds = dict(highest_confidence=Q4_epochs, high_confidence=Q3_epochs, low_confidence=Q2_epochs, lowest_confidence=Q1_epochs)\n",
    "    #     mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "\n",
    "    #     # downsampled plot\n",
    "    #     epochs = epochs.decimate(20)\n",
    "    #     Q1_epochs = epochs['confidence_quartile == %i' % 1].average()\n",
    "    #     Q2_epochs = epochs['confidence_quartile == %i' % 2].average()\n",
    "    #     Q3_epochs = epochs['confidence_quartile == %i' % 3].average()\n",
    "    #     Q4_epochs = epochs['confidence_quartile == %i' % 4].average()\n",
    "    #     evokeds = dict(highest_confidence=Q4_epochs, high_confidence=Q3_epochs, low_confidence=Q2_epochs, lowest_confidence=Q1_epochs)\n",
    "    #     mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "\n",
    "\n",
    "\n",
    "    #     # plot high vs low confidence z-scored by partner at the channel that max correlates with confidence\n",
    "    #     zscore = lambda x: (x - x.mean()) / x.std()\n",
    "    #     epochs.metadata['confidence_z_by_partner'] = epochs.metadata['participant_confidence'].groupby(epochs.metadata['partner']).transform(zscore)\n",
    "    #     median_confidence = epochs.metadata['confidence_z_by_partner'].median()\n",
    "\n",
    "    #     low_conf_epochs = epochs['confidence_z_by_partner <= %i' % median_confidence]\n",
    "    #     high_conf_epochs = epochs['confidence_z_by_partner > %i' % median_confidence]\n",
    "    #     low_conf_evoked = low_conf_epochs.average()\n",
    "    #     high_conf_evoked = high_conf_epochs.average()\n",
    "    #     evokeds = dict(high_confidence=high_conf_evoked, low_confidence=low_conf_evoked)\n",
    "    #     mne.viz.plot_compare_evokeds(evokeds, picks=[epochs.ch_names[max_index]], invert_y=False)\n",
    "\n",
    "\n",
    "    #     # compute correlation of every channel with confidence in even trials\n",
    "    #     all_corrs_even = []\n",
    "    #     for channel in range (X_even.shape[1]):\n",
    "    #         x_even = X_even[:, channel]\n",
    "    #         cor_even = np.corrcoef(y_even, x_even)[0, 1]\n",
    "    #         all_corrs_even.append(cor_even)\n",
    "    #     max_index_even = np.argmax(all_corrs_even)\n",
    "    #     print(\"\\n\\n For even trials, maximum correlation with confidence is at %s with %.3f\" %(epochs.ch_names[max_index_even], max(all_corrs_even)))\n",
    "    #     cor_odd = np.correlate(y_odd, X_odd[:, max_index_even])\n",
    "    #     print(\"\\n For odd trials, correlation with confidence at %s is %.3f\" %(epochs.ch_names[max_index_even], cor_odd))\n",
    "\n",
    "\n",
    "    #     # compute correlation of every channel with confidence in odd trials\n",
    "    #     all_corrs_odd = []\n",
    "    #     for channel in range (X_odd.shape[1]):\n",
    "    #         x_odd = X_odd[:, channel]\n",
    "    #         cor_odd = np.corrcoef(y_odd, x_odd)[0, 1]\n",
    "    #         all_corrs_odd.append(cor_odd)\n",
    "    #     max_index_odd = np.argmax(all_corrs_odd)\n",
    "    #     print(\"\\n\\n For odd trials, maximum correlation with confidence is at %s with %.3f\" %(epochs.ch_names[max_index_odd], max(all_corrs_odd)))\n",
    "    #     cor_even = np.correlate(y_even, X_even[:, max_index_odd])\n",
    "    #     print(\"\\n For even trials, correlation with confidence at %s is %.3f\" %(epochs.ch_names[max_index_odd], cor_even))\n",
    "\n",
    "\n",
    "    participant_correlations = np.array(participant_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40016e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_participant_correlations = np.mean(participant_correlations, 0)\n",
    "average_participant_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7fd40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.max(average_participant_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "print(\"\\n Average correlation of each channel with confidence across participants\")\n",
    "mne.viz.plot_topomap(data=average_participant_correlations , pos=epochs.info, names=epochs.ch_names, show_names=True)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97827398",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest = epochs.ch_names[average_participant_correlations.argsort()[-1]]\n",
    "largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e049d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_largest = epochs.ch_names[average_participant_correlations.argsort()[-2]]\n",
    "second_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49befa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_largest = epochs.ch_names[average_participant_correlations.argsort()[-3]]\n",
    "third_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_largest = epochs.ch_names[average_participant_correlations.argsort()[-4]]\n",
    "fourth_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b13795",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_largest = epochs.ch_names[average_participant_correlations.argsort()[-5]]\n",
    "fifth_largest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2fefa",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76a861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "participant_files = []\n",
    "\n",
    "\n",
    "for session in sessions:\n",
    "#     # accuracy >=62%\n",
    "#     if session == 1:\n",
    "#         participant_numbers = [1, 19, 11, 3, 17, 20, 18, 6, 2]\n",
    "#     if session == 2:\n",
    "#         participant_numbers = [2, 18, 7, 19, 1, 11, 17, 9]\n",
    "        \n",
    "    # accuracy >=64%\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3, 20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18, 7, 19, 1, 11, 17]\n",
    "        \n",
    "\n",
    "    for subject in participant_numbers:   \n",
    "        participant_files.append('%i_%i' % (subject, session))\n",
    "\n",
    "        \n",
    "remerge = True\n",
    "if remerge:\n",
    "    epochs = load_all_eeg(path='%s/' % input_dir, files=participant_files)\n",
    "    epochs.save('mergedData/stimulus_epoch_mastoids-epo.fif', overwrite=True)\n",
    "else:\n",
    "    epochs = mne.read_epochs('mergedData/stimulus_epoch_mastoids-epo.fif')\n",
    "\n",
    "data = epochs.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf805f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "# zscore by participant\n",
    "#epochs.metadata['confidence_z_by_participant'] = epochs.metadata['participant_confidence'].groupby(epochs.metadata['participant']).transform(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategic_epochs = epochs['condition == \"s\"']\n",
    "strategic_epochs.metadata['confidence_z_by_participant'] = strategic_epochs.metadata['participant_confidence'].groupby(strategic_epochs.metadata['participant']).transform(zscore)\n",
    "\n",
    "nonstrategic_epochs = epochs['condition == \"ns\"']\n",
    "nonstrategic_epochs.metadata['confidence_z_by_participant'] = nonstrategic_epochs.metadata['participant_confidence'].groupby(nonstrategic_epochs.metadata['participant']).transform(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "underconf_strategic_epochs = strategic_epochs['partner == \"underconfident\"']\n",
    "underconf_strategic_epochs.metadata['confidence_z_by_participant_by_partner'] = underconf_strategic_epochs.metadata['participant_confidence'].groupby(underconf_strategic_epochs.metadata['participant']).transform(zscore)\n",
    "\n",
    "overconf_strategic_epochs = strategic_epochs['partner == \"overconfident\"']\n",
    "overconf_strategic_epochs.metadata['confidence_z_by_participant_by_partner'] = overconf_strategic_epochs.metadata['participant_confidence'].groupby(overconf_strategic_epochs.metadata['participant']).transform(zscore)\n",
    "\n",
    "\n",
    "\n",
    "underconf_nonstrategic_epochs = nonstrategic_epochs['partner == \"underconfident\"']\n",
    "underconf_nonstrategic_epochs.metadata['confidence_z_by_participant_by_partner'] = underconf_nonstrategic_epochs.metadata['participant_confidence'].groupby(underconf_nonstrategic_epochs.metadata['participant']).transform(zscore)\n",
    "\n",
    "overconf_nonstrategic_epochs = nonstrategic_epochs['partner == \"overconfident\"']\n",
    "overconf_nonstrategic_epochs.metadata['confidence_z_by_participant_by_partner'] = overconf_nonstrategic_epochs.metadata['participant_confidence'].groupby(overconf_nonstrategic_epochs.metadata['participant']).transform(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = [\"Pz\", \"CPz\", \"POz\", \"P1\", \"P2\"]\n",
    "#roi_1 = [\"Pz\", \"CPz\", \"POz\", \"P1\", \"P2\", \"Cz\"]\n",
    "#roi_2 = [\"P3\", \"CP3\", \"P1\", \"CP1\", \"Pz\", \"CPz\", \"P2\", \"CP2\", \"P4\", \"CP4\"]\n",
    "highest_corrs = [largest, second_largest, third_largest, fourth_largest, fifth_largest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e8162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evokeds = dict(erp=list(epochs['stimulus_left_correct', 'stimulus_right_correct'].iter_evoked()))\n",
    "color_dict = {'erp':'orange'}\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, colors=color_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d60666",
   "metadata": {},
   "outputs": [],
   "source": [
    "partner_color_dict = {'underconfident_partner':'#4682B4', 'overconfident_partner':'#7fad1f'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93807f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## STRATEGIC\n",
    "grand_average_underconf = []\n",
    "grand_average_overconf = []\n",
    "\n",
    "\n",
    "for session in sessions:\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18]\n",
    "        \n",
    "    for subject in participant_numbers:    \n",
    "        grand_average_underconf.append(underconf_strategic_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf.append(overconf_strategic_epochs['participant == %i' % subject].average())\n",
    "\n",
    "\n",
    "evokeds = dict(underconfident_partner=grand_average_underconf,\n",
    "               overconfident_partner=grand_average_overconf)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, invert_y=False, colors=partner_color_dict, \n",
    "                             ci=False, ylim=dict(eeg=[-4, 9]), vlines=[epoch_start, epoch_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STRATEGIC\n",
    "\n",
    "grand_average_underconf_high_conf = []\n",
    "grand_average_underconf_low_conf = []\n",
    "grand_average_overconf_high_conf = []\n",
    "grand_average_overconf_low_conf = []\n",
    "\n",
    "underconf_high_conf_epochs = underconf_strategic_epochs['confidence_z_by_participant >= 0']\n",
    "underconf_low_conf_epochs = underconf_strategic_epochs['confidence_z_by_participant < 0']\n",
    "\n",
    "overconf_high_conf_epochs = overconf_strategic_epochs['confidence_z_by_participant >= 0']\n",
    "overconf_low_conf_epochs = overconf_strategic_epochs['confidence_z_by_participant < 0']\n",
    "\n",
    "for session in sessions:\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18]\n",
    "\n",
    "    for subject in participant_numbers:    \n",
    "        grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    \n",
    "\n",
    "evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "               underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "               overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "               overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, invert_y=False, ci=False, ylim=dict(eeg=[-4, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf04f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NON-STRATEGIC\n",
    "grand_average_underconf = []\n",
    "grand_average_overconf = []\n",
    "\n",
    "for session in sessions:\n",
    "    if session == 1:\n",
    "        participant_numbers = [20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [7, 19, 1, 11, 17]\n",
    "        \n",
    "    for subject in participant_numbers:    \n",
    "        grand_average_underconf.append(underconf_nonstrategic_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf.append(overconf_nonstrategic_epochs['participant == %i' % subject].average())\n",
    "\n",
    "evokeds = dict(underconfident_partner=grand_average_underconf,\n",
    "               overconfident_partner=grand_average_overconf)\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, invert_y=False, colors=partner_color_dict, \n",
    "                             ci=False, ylim=dict(eeg=[-4, 9]), vlines=[epoch_start, epoch_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-STRATEGIC \n",
    "\n",
    "grand_average_underconf_high_conf = []\n",
    "grand_average_underconf_low_conf = []\n",
    "grand_average_overconf_high_conf = []\n",
    "grand_average_overconf_low_conf = []\n",
    "\n",
    "underconf_high_conf_epochs = underconf_nonstrategic_epochs['confidence_z_by_participant > 0']\n",
    "underconf_low_conf_epochs = underconf_nonstrategic_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "overconf_high_conf_epochs = overconf_nonstrategic_epochs['confidence_z_by_participant > 0']\n",
    "overconf_low_conf_epochs = overconf_nonstrategic_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "for session in sessions:\n",
    "    if session == 1:\n",
    "        participant_numbers = [20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [7, 19, 1, 11, 17]\n",
    "        \n",
    "    for subject in participant_numbers:    \n",
    "        grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "\n",
    "\n",
    "evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "               underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "               overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "               overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, invert_y=False, ci=False, ylim=dict(eeg=[-4, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STRATEGIC\n",
    "\n",
    "grand_average_underconf_high_conf = []\n",
    "grand_average_underconf_low_conf = []\n",
    "grand_average_overconf_high_conf = []\n",
    "grand_average_overconf_low_conf = []\n",
    "\n",
    "underconf_high_conf_epochs = underconf_strategic_epochs['confidence_z_by_participant >= 0']\n",
    "underconf_low_conf_epochs = underconf_strategic_epochs['confidence_z_by_participant < 0']\n",
    "\n",
    "overconf_high_conf_epochs = overconf_strategic_epochs['confidence_z_by_participant >= 0']\n",
    "overconf_low_conf_epochs = overconf_strategic_epochs['confidence_z_by_participant < 0']\n",
    "\n",
    "\n",
    "\n",
    "for session in sessions:\n",
    "    if session == 1:\n",
    "        participant_numbers = [1, 19, 11, 3]\n",
    "    if session == 2:\n",
    "        participant_numbers = [2, 18]\n",
    "\n",
    "    for subject in participant_numbers:    \n",
    "        grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "    \n",
    "\n",
    "# # plot with 95% confidence intervals\n",
    "# evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "#                underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "#                overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "#                overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "# mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=roi, invert_y=False)\n",
    "\n",
    "\n",
    "\n",
    "# plot without CIs\n",
    "grand_average_underconf_high_conf = mne.grand_average(grand_average_underconf_high_conf)\n",
    "grand_average_underconf_low_conf = mne.grand_average(grand_average_underconf_low_conf)\n",
    "grand_average_overconf_high_conf = mne.grand_average(grand_average_overconf_high_conf)\n",
    "grand_average_overconf_low_conf = mne.grand_average(grand_average_overconf_low_conf)\n",
    "\n",
    "evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "               underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "               overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "               overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "print(\"\\n\\n strategic task, Pz\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=['Pz'], invert_y=False)\n",
    "print(\"\\n\\n strategic task, roi\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=roi, invert_y=False)\n",
    "print(\"\\n\\n strategic task, highest correlating electrodes\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=highest_corrs, invert_y=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69321f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-STRATEGIC \n",
    "\n",
    "grand_average_underconf_high_conf = []\n",
    "grand_average_underconf_low_conf = []\n",
    "grand_average_overconf_high_conf = []\n",
    "grand_average_overconf_low_conf = []\n",
    "\n",
    "underconf_high_conf_epochs = underconf_nonstrategic_epochs['confidence_z_by_participant > 0']\n",
    "underconf_low_conf_epochs = underconf_nonstrategic_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "overconf_high_conf_epochs = overconf_nonstrategic_epochs['confidence_z_by_participant > 0']\n",
    "overconf_low_conf_epochs = overconf_nonstrategic_epochs['confidence_z_by_participant <= 0']\n",
    "\n",
    "\n",
    "for session in sessions:\n",
    "    if session == 1:\n",
    "        participant_numbers = [20]\n",
    "    if session == 2:\n",
    "        participant_numbers = [7, 19, 1, 11, 17]\n",
    "\n",
    "    for subject in participant_numbers:      \n",
    "        grand_average_underconf_high_conf.append(underconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_underconf_low_conf.append(underconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_high_conf.append(overconf_high_conf_epochs['participant == %i' % subject].average())\n",
    "        grand_average_overconf_low_conf.append(overconf_low_conf_epochs['participant == %i' % subject].average())\n",
    "\n",
    "\n",
    "# # plot with 95% confidence intervals\n",
    "# evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "#                underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "#                overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "#                overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "# mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['Pz'], invert_y=False)\n",
    "\n",
    "\n",
    "\n",
    "# plot without CIs\n",
    "grand_average_underconf_high_conf = mne.grand_average(grand_average_underconf_high_conf)\n",
    "grand_average_underconf_low_conf = mne.grand_average(grand_average_underconf_low_conf)\n",
    "grand_average_overconf_high_conf = mne.grand_average(grand_average_overconf_high_conf)\n",
    "grand_average_overconf_low_conf = mne.grand_average(grand_average_overconf_low_conf)\n",
    "\n",
    "evokeds = dict(underconf_low_confidence=grand_average_underconf_low_conf,\n",
    "               underconf_high_confidence=grand_average_underconf_high_conf,\n",
    "               overconf_low_confidence=grand_average_overconf_low_conf,\n",
    "               overconf_high_confidence=grand_average_overconf_high_conf)\n",
    "\n",
    "print(\"\\n\\n non-strategic task, Pz\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=['Pz'], invert_y=False)\n",
    "print(\"\\n\\n non-strategic task, roi\")\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks=roi, invert_y=False)\n",
    "#print(\"\\n\\n non-strategic task, highest correlating electrodes\")\n",
    "#mne.viz.plot_compare_evokeds(evokeds, picks=highest_corrs, invert_y=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a122134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c9aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f3223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
